{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbbc4c9",
   "metadata": {},
   "source": [
    "(hw1_models)=\n",
    "\n",
    "# HW 1 part 2\n",
    "\n",
    ":::{epigraph}\n",
    "Linear Regression: Models\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa2ef8",
   "metadata": {},
   "source": [
    "## Part 2 Table of Contents and Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|------------------------------------|-------|\n",
    "| Linear Regression Implementation | 1.5 |\n",
    "| Analysis | 2 |\n",
    "| Ethics | 1.5 |\n",
    "| Reflection | 0.5 |\n",
    "| Total | 5.5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f35fdc",
   "metadata": {},
   "source": [
    "# Notebook and function imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d4f35",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "If you click on the vertical blue bar on the left of a cell, you can collapse the code which can help organize the notebook as you work through the project.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6cd8e",
   "metadata": {},
   "source": [
    "If you have tested your implementation in Part 1 against the autograder, you would have generated a file called `hw1_foundations.py`. Let's now import those functions into this notebook for use in Part 2. \n",
    "\n",
    "If you are running this notebook on the JupyterHub allocated for the course:\n",
    "\n",
    "1. Open the file browser by going to the menu bar \"View -> File Browser\"\n",
    "2. Navigate to `comsc335.github.io/hws/`, you should see your `hw1_models.ipynb` file in that folder\n",
    "3. Click on the upload button in the upper right and upload the `hw1_foundations.py` file to this directory\n",
    "4. Run the following cell below to import the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import Self\n",
    "\n",
    "from hw1_foundations import linreg_grad_descent, mse_loss\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288c279",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a5c0f",
   "metadata": {},
   "source": [
    "# 3. Linear regression model class [1.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05968226",
   "metadata": {},
   "source": [
    "Let's put the gradient descent implementation together and create a linear regression model class. Following the same pattern as we did in Worksheet 1, we'll create a class that inherits from scikit-learn's `BaseEstimator` and implements the `fit` and `predict` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e974b6",
   "metadata": {},
   "source": [
    ":::{admonition} ML model class documentation\n",
    "\n",
    "As Guido van Rossum, the creator of Python, likes to say:\n",
    "\n",
    "> Code is read much more often than it is written.\n",
    "\n",
    "Part of growing as a computer scientist or data scientist is to be able to effectively communicate your implementation to others.\n",
    "\n",
    "As such, a part of the homework assignments will be completing the documentation of the machine learning model classes you implement in this course.\n",
    "\n",
    "Please make sure that you document every method parameter, and provide descriptions of the class and its methods.\n",
    "\n",
    "The docstrings shown in the methods in Part 1 and in Worksheet 1 follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html#s3.8-comments-and-docstrings), and you can use them as examples for your own documentation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9412ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHCLinearRegressor(BaseEstimator):\n",
    "    \"\"\"TODO class description\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float, max_iters: int=5000):\n",
    "        \"\"\"TODO constructor description\"\"\"\n",
    "        \n",
    "        # TODO initialize the hyperparameters of alpha and max_iters\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> Self:\n",
    "        \"\"\"TODO method description\"\"\"\n",
    "\n",
    "        # TODO save the fitted weights, loss values, and weight history by\n",
    "        # calling the linreg_grad_descent function with the correct parameters\n",
    "        self.weights_, self.loss_values_, self.w_history_ = None, None, None\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"TODO method description\"\"\"\n",
    "\n",
    "        # TODO use the fitted weights to make predictions on the input data\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize some simple data for testing, n=3, p=2\n",
    "    X = np.array([[1, 2], \n",
    "                  [2, 3], \n",
    "                  [3, 3]])\n",
    "    y = np.array([0, 1, 2])\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Test the linear regression model\n",
    "    model = MHCLinearRegressor(alpha=0.05)\n",
    "    model = model.fit(X, y)\n",
    "    assert model is not None, \"The model should be fitted and returned in fit()\"\n",
    "    assert np.allclose(model.predict(X), y, atol=1e-3), \"The predictions should be equal to the y targets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca46151",
   "metadata": {},
   "source": [
    "# 4. Analysis [2 pts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83848f0b",
   "metadata": {},
   "source": [
    "## 4.1 Gradient descent alpha simulation [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694b3f3",
   "metadata": {},
   "source": [
    "You'll now use your newly implemented `MHCLinearRegressor` class to explore the effect of the learning rate $\\alpha$ on the gradient descent convergence.\n",
    "\n",
    "Run the code cell below to see an interactive plot of the gradient descent algorithm path for fitting your `MHCLinearRegressor` model with different values of $\\alpha$. The plot shows the contour plot along with how the weights update at each iteration (represented by the smaller white circles), with the title showing the final MSE loss and the number of iterations needed to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073a957",
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys; sys.path.insert(0, '..')\n",
    "    from utils import explore_alpha\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "    widgets.interact_manual(explore_alpha, \n",
    "        # Tells the widget to use the MHCLinearRegressor class\n",
    "        LinModel=widgets.fixed(MHCLinearRegressor), \n",
    "        # Creates an interactive slider for the learning rate\n",
    "        alpha=widgets.FloatSlider(value=0.1, min=0.1, max=0.66, step=0.05)\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3d976",
   "metadata": {},
   "source": [
    "4.1.1: Increase the learning rate by increments of 0.05 from 0.1 to 0.65. Comment on what you see in both the number of iterations needed to converge as well as the \"shape\" of the gradient descent path as $\\alpha$ increases.\n",
    "\n",
    "4.1.2: Now, slide the learning rate all the way to 0.66. What do you observe in the ending loss value and the start and end points of the gradient descent path?\n",
    "\n",
    "4.1.3: Summarize the tradeoffs you see between setting a high vs low $\\alpha$, and propose a potential strategy for picking $\\alpha$ in practice (It's okay to speculate here as long as you provide a rationale, as this is an open-ended question).\n",
    "\n",
    "**TODO your responses:**\n",
    "\n",
    "4.1.1:\n",
    "\n",
    "4.1.2:\n",
    "\n",
    "4.1.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fae05",
   "metadata": {},
   "source": [
    "## 4.2 Bikeshare regression and feature selection [1 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205d8dd",
   "metadata": {},
   "source": [
    "Let's now conduct a regression prediction task using bikeshare information. We can imagine that we're machine learning engineers working for a bikeshare company, where our goal is to predict the number of daily bike rentals $y$ to better model demand and plan for maintenance. Each row of the dataset contains weather and time-based information on a given day:\n",
    "\n",
    "**Predictors**:\n",
    "\n",
    "- `workingday`: Is it a work day? Yes=1, No=0.\n",
    "- `temp`: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39.\n",
    "- `hum`: Normalized humidity. The values are divided to 100 (max).\n",
    "- `windspeed`: Normalized wind speed. The values are divided by 67 (max).\n",
    "\n",
    "**Target**:\n",
    "\n",
    "- `bikers`: the number of bike rentals in a day.\n",
    "\n",
    ":::{admonition} Normalized features\n",
    "\n",
    "As a data **preprocessing** step, the dataset creators *normalized* the weather features, which means that all of the values are scaled to be between 0 and 1. This aids in the model fitting process and with the interpretation of the coefficients, as all of the features are in the same range. We will discuss data preprocessing principles in more detail in upcoming classes.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a1fa6",
   "metadata": {},
   "source": [
    "We'll first perform a simple feature selection task. Our goal in machine learning is to fit a model on the **training data** so that it has low loss on new data. The new data is often called the **test set**. Since our `MHCLinearRegressor` model can take data with two features, we'll have to select two of the four features to use in our model. We have provided starter code below that helps load in and create the `X_train_all`, `X_test_all`, `y_train`, and `y_test` NumPy arrays. It then iterates over all possible combinations of two features to include in the model.\n",
    "\n",
    "\n",
    "Complete the code below that:\n",
    "\n",
    "1. Trains a `MHCLinearRegressor` model on the selected training data, using `alpha=0.05`.\n",
    "2. Predicts the testing data using the trained model.\n",
    "3. Computes the loss of the model on the testing data, and extracts the weights of the model.\n",
    "\n",
    "Which model achieves the lowest test set MSE loss, and which two features does it use?\n",
    "\n",
    "- Lowest test MSE loss: **TODO**\n",
    "- Features used: **TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Loads in the bikeshare data\n",
    "    data = np.load(\"../COMSC-335/data/bikeshare_hw1.npz\", allow_pickle=True)\n",
    "\n",
    "    X_train_all = data[\"X_train\"]   # shape (n_train, 4)\n",
    "    y_train = data[\"y_train\"]       # shape (n_train,)\n",
    "    X_test_all = data[\"X_test\"]     # shape (n_test, 4)\n",
    "    y_test = data[\"y_test\"]         # shape (n_test,)\n",
    "\n",
    "    # Feature names: [\"workingday\", \"temp\", \"hum\", \"windspeed\"]\n",
    "    feature_names = data[\"feature_names\"].tolist()  \n",
    "\n",
    "    # Maps the feature names to their indices\n",
    "    name_to_idx = {name: i for i, name in enumerate(feature_names)}\n",
    "\n",
    "    # All possible pairs of features\n",
    "    pairs = [\n",
    "        (\"workingday\", \"temp\"),\n",
    "        (\"workingday\", \"hum\"),\n",
    "        (\"workingday\", \"windspeed\"),\n",
    "        (\"temp\", \"hum\"),\n",
    "        (\"temp\", \"windspeed\"),\n",
    "        (\"hum\", \"windspeed\"),\n",
    "    ]\n",
    "\n",
    "    for feat1, feat2 in pairs:\n",
    "        # Select the columns corresponding to the two features\n",
    "        i, j = name_to_idx[feat1], name_to_idx[feat2]\n",
    "\n",
    "        # Create the training and testing data\n",
    "        X_train = X_train_all[:, [i, j]]\n",
    "        X_test = X_test_all[:, [i, j]]\n",
    "\n",
    "        # TODO Initialize the model\n",
    "        model = None\n",
    "\n",
    "        # TODO train the model using X_train and y_train\n",
    "\n",
    "\n",
    "        # TODO predict on X_test\n",
    "\n",
    "\n",
    "        # TODO compute the loss of the model compared to y_test using your mse_loss function\n",
    "        test_mse = 0\n",
    "\n",
    "        # Extract the weights from the model object\n",
    "        w0, w1, w2 = model.weights_\n",
    "\n",
    "        # Prints the results for each feature pair\n",
    "        print(f\"Model: ({feat1}, {feat2})\")\n",
    "        print(f\"\\ttest MSE: {test_mse:.3f}\")\n",
    "        print(f\"\\tweights: {feat1}={w1:.3f}, {feat2}={w2:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cdd62",
   "metadata": {},
   "source": [
    "## 4.3 Bikeshare interpretation [0.5 pts]\n",
    "\n",
    "As we discussed in class, we sometimes take the square root of the MSE (abbreviated as RMSE for Root Mean Square Error) to better interpret the model performance. By taking the square root, we convert the units of the loss back to the original units of the target variable. Compute the RMSE of the best model from the previous question, and provide an interpretation of the model's performance in words.\n",
    "\n",
    "**TODO** your response here:\n",
    "\n",
    "Let's also interpret the fitted weights of the best model from above. For both of the two feature weights, when the feature increases, does it **increase** or **decrease** the predicted number of bike rentals? Give a brief explanation of whether you think the signs of the weights make intuitive sense.\n",
    "\n",
    "- **TODO** feature 1: \n",
    "- **TODO** feature 2: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0477d4",
   "metadata": {},
   "source": [
    ":::{admonition} RMSE check\n",
    ":class: dropdown\n",
    "\n",
    "You should see that the model with the lowest test MSE loss should be approximately in the ~100-150 range.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b341a1",
   "metadata": {},
   "source": [
    "# 5. \"Data Neutrality\" Ethics [1.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531034d6",
   "metadata": {},
   "source": [
    "Alongside exploring the technical foundations of machine learning, we will also consider the ethics of machine learning within socio-technical systems. Here, we will examine the notion of \"data neutrality.\"\n",
    "\n",
    ":::{admonition}\n",
    "\n",
    "Read [\"Data is never a raw, truthful input, and it is never neutral\"](https://www.theguardian.com/technology/2020/mar/21/catherine-dignazio-data-is-never-a-raw-truthful-input-and-it-is-never-neutral) and answer the discussion questions below.\n",
    "\n",
    ":::\n",
    "\n",
    "<!--\n",
    "TODO For written responses, support your thoughts with evidence from the text. Write concisely but clearly -- one-word/one-phrase answers usually don't give enough space to show what you've learned.\n",
    "-->\n",
    "\n",
    "**5.1:** When the author says \"data is never neutral,\" what do they mean?\n",
    "\n",
    "**5.2:** What are \"who questions\"? Why does the author advocate for using them?\n",
    "\n",
    "**5.3:** Discuss who you believe has the responsibility to ask the \"who questions.\" Is it the person who funds the research? The researcher or engineer who collected the data? The data scientist or computer scientist who analyzed it? Other parties?\n",
    "\n",
    "**5.4:** The creators of the bikeshare dataset included the following description:\n",
    "\n",
    "> Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. \n",
    ">\n",
    "> Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\n",
    "> \n",
    "> The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. We aggregated the data on a daily basis and then extracted and added the corresponding weather and seasonal information. Weather information are extracted from http://www.freemeteo.com. \n",
    "\n",
    "Also note that Lyft bought the parent company of Capital Bikeshare in 2018. Building on the interview article, reflect on *who* benefits from using this dataset and a machine learning model that predicts bike rentals to guide decisions (e.g. where new stations go, how many bikes to stock, pricing changes, etc.) as well as *who* could be potentially harmed or overlooked, and why. Discuss these in brief paragraphs (~2-3 sentences) each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b21eca",
   "metadata": {},
   "source": [
    "\n",
    "**TODO your responses**:\n",
    "\n",
    "5.1:\n",
    "\n",
    "5.2:\n",
    "\n",
    "5.3:\n",
    "\n",
    "5.4: \n",
    "\n",
    "- Potential benefits: \n",
    "\n",
    "- Potential harms:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca367e4c",
   "metadata": {},
   "source": [
    "## 6. Reflection [0.5 pts]\n",
    "\n",
    "1. How much time did you spend on this assignment?\n",
    "\n",
    "2. Were there any parts of the assignment that you found particularly challenging?\n",
    "\n",
    "3. What is one thing you have a better understanding of after completing this assignment and going through the class content?\n",
    "\n",
    "4. Do you have any follow-up questions about concepts that you'd like to explore further?\n",
    "\n",
    "5. Indicate the number of late days (if any) you are using for this assignment.\n",
    "\n",
    "**TODO** your responses here:\n",
    "\n",
    "6.1:\n",
    "\n",
    "6.2:\n",
    "\n",
    "6.3:\n",
    "\n",
    "6.4:\n",
    "\n",
    "6.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e6b16",
   "metadata": {},
   "source": [
    ":::{admonition} How to submit\n",
    ":class: tip\n",
    "\n",
    "\n",
    "Like Worksheet 1, follow the instructions on the [course website](https://comsc335.github.io/syllabus/submit.html) to submit your work. For all of Homework 1, your submission will include the files from both parts:\n",
    "- `hw1_foundations.ipynb` and `hw1_foundations.py`\n",
    "- `hw1_models.ipynb` and `hw1_models.py`\n",
    " \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce7305",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "- The bikeshare dataset is sourced from the [ISLP repository](https://islp.readthedocs.io/en/latest/datasets/Bikeshare.html)\n",
    "- The data ethics exercise is sourced from Yaniv Yacoby's [Probabilistic Foundations of ML course](https://mogu-lab.github.io/probabilistic-foundations-of-ml/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
