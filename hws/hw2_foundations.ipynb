{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(hw2_foundations)=\n",
    "\n",
    "# HW 2 part 1\n",
    "\n",
    ":::{epigraph}\n",
    "Classification: Foundations\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Table of Contents and Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|------------------------------------|-------|\n",
    "| Deriving Gradients for Logistic Regression | 1.5 |\n",
    "| Logistic Regression Gradient Descent Update | 1.5 |\n",
    "| Logistic Regression Model Class | 2 |\n",
    "| Total | 5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook and function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# random number generator\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deriving Gradients for Logistic Regression [1.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll derive the gradient update rules for logistic regression.\n",
    "\n",
    "From class, we saw that logistic regression uses a two-step model to produce predicted probabilities:\n",
    "\n",
    "**Step 1:** Compute the linear combination $h_i$:\n",
    "\n",
    "$$\n",
    "h_i = w_0 + w_1 x_{i,1} + w_2 x_{i,2} + \\cdots + w_p x_{i,p}\n",
    "$$\n",
    "\n",
    "**Step 2:** Pass $h_i$ through the sigmoid function $\\sigma$ to produce a predicted probability:\n",
    "\n",
    "$$\n",
    "\\sigma(h_i) = \\frac{1}{1 + e^{-h_i}}\n",
    "$$\n",
    "\n",
    "We pair this model with the **log loss** (also called binary cross-entropy) for a single data point $(\\vec{x}_i, y_i)$:\n",
    "\n",
    "$$\n",
    "\\ell_{\\log, i} = -y_i \\log(\\sigma(h_i)) - (1 - y_i) \\log(1 - \\sigma(h_i))\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition} Sigmoid derivative identity\n",
    ":class: tip\n",
    "To compute the gradients of the log loss, we'll need the following identity for the derivative of the sigmoid function (if you enjoy algebra, feel free to try deriving it yourself!):\n",
    "\n",
    "$$\n",
    "\\frac{d}{dh}\\sigma(h) = \\sigma(h)(1 - \\sigma(h))\n",
    "$$\n",
    "\n",
    "We'll use this identity throughout the derivations below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Some helpful derivatives [0.5 pts]\n",
    "\n",
    "We first compute a couple of derivatives that will be helpful for the log loss derivation. Show that:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dh}\\log(\\sigma(h)) = 1 - \\sigma(h)\n",
    "$$\n",
    "\n",
    ":::{admonition} Calculus hints and reminders\n",
    ":class: tip\n",
    "\n",
    "The derivative of $\\log(x)$ is $\\frac{1}{x}$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\log(x) = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "You'll then need to use the chain rule and substitute the sigmoid derivative identity from above. You don't need to take the derivative of $\\sigma(h)$ from scratch!\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your response:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dh}\\log(\\sigma(h)) &= \\text{ TODO step 1 } \\\\\n",
    "&= \\text{ TODO step 2 } \\\\\n",
    "&= 1 - \\sigma(h)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, also show that:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dh}\\log(1 - \\sigma(h)) = -\\sigma(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your response:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dh}\\log(1 - \\sigma(h)) &= \\text{ TODO step 1 } \\\\\n",
    "&= \\text{ TODO step 2 } \\\\\n",
    "&= -\\sigma(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Gradient of log loss with respect to $w_0$ [0.5 pts]\n",
    "\n",
    "Using your results from 1.1, derive the partial derivative of the single-point log loss with respect to $w_0$:\n",
    "\n",
    "$$\\ell_{\\log, i} = -y_i \\log(\\sigma(h_i)) - (1 - y_i) \\log(1 - \\sigma(h_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your response (you can just write the final expression):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_{\\log, i}}{\\partial w_0} = \\text{ TODO }\n",
    "$$\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "Use the chain rule in two steps: first differentiate $\\ell_i$ with respect to $h_i$ (using your results from 1.1), then differentiate $h_i$ with respect to $w_0$. \n",
    "\n",
    "The final result should be quite simple after some cancellation: just two terms.\n",
    "\n",
    "If you get stuck at any point of this derivation, please don't hesitate to reach out on Ed or come to TA/office hours!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gradient of log loss with respect to $w_j$ [0.5 pts]\n",
    "\n",
    "Now derive the partial derivative of the single-point log loss $\\ell_i$ with respect to $w_j$ (for $j \\in \\{1, 2, \\ldots, p\\}$). \n",
    "\n",
    "Your response:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_{\\text{log}, i}}{\\partial w_j} = \\text{ TODO }\n",
    "$$\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This should be very similar to 1.2, with the only difference being what $\\frac{\\partial h_i}{\\partial w_j}$ is as opposed to $\\frac{\\partial h_i}{\\partial w_0}$. The final result should have three terms.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Gradient Descent for Logistic Regression [1.5 pts]\n",
    "\n",
    "Now, let's translate the math into code to build our own logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sigmoid function [0.25 pts]\n",
    "\n",
    "We begin by implementing the sigmoid function that we saw in class:\n",
    "\n",
    "$$\n",
    "\\sigma(h) = \\frac{1}{1 + e^{-h}}\n",
    "$$\n",
    "\n",
    "In NumPy, $e^{h}$ is computed as `np.exp(h)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Python Type Hint Unions\n",
    "\n",
    "You might notice that the `sigmoid()` function has a type hint that looks like:\n",
    "\n",
    "```python\n",
    "def sigmoid(h: np.ndarray | float)\n",
    "```\n",
    "The `np.ndarray | float` indicates that the function can take in either a numpy array or a scalar. This is an example of a [type hint union](https://docs.python.org/3/library/typing.html#type-hinting-unions), which indicates that multiple types are acceptable for the function parameter.\n",
    "\n",
    "To be clear, you don't need to do anything special in your implementation to handle this. A single line of code with `np.exp` will work for both scalar and array inputs!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(h: np.ndarray | float) -> np.ndarray | float:\n",
    "    \"\"\"Compute the sigmoid function.\n",
    "\n",
    "    Args:\n",
    "        h: a scalar or numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "        The sigmoid applied element-wise, same shape as input\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test sigmoid with scalar input\n",
    "    assert np.isclose(sigmoid(0), 0.5), \"sigmoid(0) should be 0.5\"\n",
    "\n",
    "    # Test sigmoid with array input\n",
    "    result = sigmoid(np.array([-1, 0, 1]))\n",
    "    expected = np.array([1 / (1 + np.exp(1)), 0.5, 1 / (1 + np.exp(-1))])\n",
    "    assert np.allclose(result, expected), f\"sigmoid([-1, 0, 1]) should be {expected}\"\n",
    "\n",
    "    # Test that sigmoid outputs are between 0 and 1\n",
    "    assert np.all(result >= 0) and np.all(result <= 1), \"sigmoid outputs should be between 0 and 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistic regression predictions [0.5 pts]\n",
    "\n",
    "Next, implement the `logreg_predictions` function that computes predicted probabilities for all examples in `X` at once.\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This should look very similar to the `linreg_predictions` function from the last exercise in Worksheet 2. The only difference is that we now pass the linear regression result into the sigmoid function.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_predictions(X: np.ndarray, w: np.ndarray, w0: float) -> np.ndarray:\n",
    "    \"\"\"Compute logistic regression predicted probabilities for all examples in X.\n",
    "\n",
    "    Args:\n",
    "        X: data examples of shape (n, p)\n",
    "        w: weights of shape (p,)\n",
    "        w0: scalar intercept term\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (n,) of predicted probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test logreg_predictions with simple data\n",
    "    X = np.array([[1, 2], [3, 4]])\n",
    "    w = np.array([0.5, -0.5])\n",
    "    w0 = 0.0\n",
    "\n",
    "    preds = logreg_predictions(X, w, w0)\n",
    "    assert preds.shape == (2,), \"Predictions should have shape (n,)\"\n",
    "\n",
    "    # h = [1*0.5 + 2*(-0.5), 3*0.5 + 4*(-0.5)] = [-0.5, -0.5]\n",
    "    # sigmoid(-0.5) = 1 / (1 + exp(0.5))\n",
    "    expected = sigmoid(np.array([-0.5, -0.5]))\n",
    "    assert np.allclose(preds, expected), f\"Predictions should be {expected}\"\n",
    "\n",
    "    # Test with zero weights: predictions should all be sigmoid(w0)\n",
    "    preds_zero = logreg_predictions(X, np.array([0.0, 0.0]), 1.0)\n",
    "    assert np.allclose(preds_zero, sigmoid(1.0)), \"With zero weights, predictions should be sigmoid(w0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Batch gradient update rule with regularization [0.75 pts]\n",
    "\n",
    "Now let's implement a single batch gradient descent step for logistic regression with **ridge (L2) regularization**. The regularized log loss with ridge over the full dataset is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ridge log}} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(-y_i \\log(\\sigma(h_i)) - (1 - y_i) \\log(1 - \\sigma(h_i))\\right) + \\lambda \\sum_{j=1}^{p} w_j^2\n",
    "$$\n",
    "\n",
    "This is just like how we added the L2 regularization term to the linear regression MSE loss function.\n",
    " \n",
    "Taking the gradients using your results from Question 1, averaged over the dataset, plus the regularization term, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^{n}\\underbrace{\\frac{\\partial \\ell_{\\log, i}}{\\partial w_0}}_{\\text{answer from 1.2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^{n}\\underbrace{\\frac{\\partial \\ell_{\\log, i}}{\\partial w_j}}_{\\text{answer from 1.3}} + 2\\lambda w_j\n",
    "$$\n",
    "\n",
    "Note that the regularization term $2\\lambda w_j$ only applies to the feature weights $w_j$, **not** to the intercept $w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now write a function to perform a single gradient descent update for the logistic regression weights, with regularization. Specifically the outputs will be:\n",
    "\n",
    "$$\n",
    "w_{0, \\text{new}} = w_{0, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_0}\n",
    "$$\n",
    "\n",
    "For each $j = 1, \\ldots, p$, we have:\n",
    "\n",
    "$$\n",
    "w_{j, \\text{new}} = w_{j, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "If we define the gradient vector as: $\\nabla \\mathcal{L}_{\\text{ridge log}} = (\\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_0}, \\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_1}, \\ldots, \\frac{\\partial \\mathcal{L}_{\\text{ridge log}}}{\\partial w_p})$, then our code can perform the gradient descent simultaneously for all the weights:\n",
    "\n",
    "$$\\vec{w}_{\\text{new}} = \\vec{w}_{\\text{old}} - \\alpha \\nabla \\mathcal{L}_{\\text{ridge log}}$$\n",
    "\n",
    "Where $\\vec{w}_{\\text{new}}$ and $\\vec{w}_{\\text{old}}$ are vectors of shape $(p,)$ holding the new and old weights, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_grad_update(w0_old: float, w_old: np.ndarray, X: np.ndarray, y: np.ndarray, alpha: float, lam: float) -> tuple:\n",
    "    \"\"\"Perform a single gradient update for logistic regression with ridge regularization.\n",
    "\n",
    "    Args:\n",
    "        w0_old: the old intercept term\n",
    "        w_old: the old weights of shape (p,)\n",
    "        X: the feature matrix of shape (n, p)\n",
    "        y: the target vector of shape (n,)\n",
    "        alpha: the learning rate\n",
    "        lam: the regularization parameter lambda\n",
    "\n",
    "    Returns:\n",
    "        (w0_new, w_new): the new intercept term and weights of shape (p,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the predicted probabilities and the difference from the targets\n",
    "    y_hat = logreg_predictions(X, w_old, w0_old)\n",
    "    diff = (y_hat - y).reshape(-1, 1)\n",
    "\n",
    "    w0_new = None\n",
    "    w_new = None\n",
    "\n",
    "    assert isinstance(w0_new, (float, np.floating)), \"The new intercept term should be a scalar\"\n",
    "    assert w_new.shape == w_old.shape, \"The new weights should have the same shape as the old weights\"\n",
    "\n",
    "    return w0_new, w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Hints\n",
    ":class: tip\n",
    "\n",
    "Use the `diff` variable to compute both updates. Both `w0_new` and `w_new` can be computed in a single line of code each, with no loops needed.\n",
    "\n",
    "For `w_new`: you'll need to broadcast `diff` against `X` to compute the gradient for all weights simultaneously. Since `X` is an `(n, p)` shape array, `diff` needs to be reshaped to a 2D array with shape `(n, 1)` via `reshape(-1, 1)` in order for the broadcasting to work:\n",
    "\n",
    "```python\n",
    "diff.reshape(-1, 1) (2d array): n x 1\n",
    "X                   (2d array): n x p\n",
    "result              (2d array): n x p\n",
    "```\n",
    "\n",
    "Don't forget to add the regularization term to `w_new` as well!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test with simple data\n",
    "    X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    y = np.array([0.0, 1.0, 1.0])\n",
    "    w_old = np.array([0.0, 0.0])\n",
    "    w0_old = 0.0\n",
    "    alpha = 0.1\n",
    "    lam = 0.0  # no regularization for this test\n",
    "\n",
    "    w0_new, w_new = logreg_grad_update(w0_old, w_old, X, y, alpha, lam)\n",
    "\n",
    "    # With zero weights and w0=0, all predictions are sigmoid(0) = 0.5\n",
    "    # diff = [0.5-0, 0.5-1, 0.5-1] = [0.5, -0.5, -0.5]\n",
    "    # mean(diff) = -1/6\n",
    "    # w0_new = 0 - 0.1 * (-1/6) = 1/60\n",
    "    assert np.isclose(w0_new, 1/60), f\"w0_new should be {1/60}, got {w0_new}\"\n",
    "\n",
    "    # diff.reshape(-1, 1) * X = [[0.5, 1.0], [-1.5, -2.0], [-2.5, -3.0]]\n",
    "    # mean over rows = [-7/6, -4/3]\n",
    "    # w_new = [0, 0] - 0.1 * [-7/6, -4/3] = [7/60, 4/30]\n",
    "    assert np.allclose(w_new, np.array([7/60, 2/15])), f\"w_new should be [7/60, 2/15], got {w_new}\"\n",
    "\n",
    "    # Test with regularization\n",
    "    w_old_reg = np.array([10, 10])\n",
    "    w0_new_reg, w_new_reg = logreg_grad_update(0.0, w_old_reg, X, y, alpha, lam=0.1)\n",
    "    assert w_new_reg.shape == (2,), \"w_new with regularization should have shape (p,)\"\n",
    "\n",
    "    assert np.allclose(w_new_reg, np.array([293/30, 146/15])), f\"w_new with regularization should be [293/30, 146/15], got {w_new_reg}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression Model Class [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a gradient descent implementation that uses your `logreg_grad_update` function from the previous question. It follows the same pattern as the one you wrote in HW 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_grad_descent(X: np.ndarray, y: np.ndarray, alpha: float, lam: float, max_iters: int = 5000) -> tuple:\n",
    "    \"\"\"Perform gradient descent for logistic regression with ridge regularization.\n",
    "\n",
    "    Args:\n",
    "        X: the feature matrix of shape (n, p)\n",
    "        y: the target vector of shape (n,)\n",
    "        alpha: the learning rate\n",
    "        lam: the regularization parameter\n",
    "        max_iters: the maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        (w0, w): the final intercept term and weights of shape (p,)\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    w0_old = 0.0\n",
    "    w_old = rng.randn(p)\n",
    "\n",
    "    # Initialize in case we hit max_iters without converging\n",
    "    w0_new, w_new = w0_old, w_old\n",
    "\n",
    "    num_iters = 0\n",
    "\n",
    "    while True:\n",
    "        w0_new, w_new = logreg_grad_update(w0_old, w_old, X, y, alpha, lam)\n",
    "\n",
    "        # Check for convergence using the full parameter vector (including intercept)\n",
    "        step_norm = np.sqrt((w0_new - w0_old) ** 2 + np.sum((w_new - w_old) ** 2))\n",
    "        if step_norm < 1e-4:\n",
    "            break\n",
    "\n",
    "        if num_iters >= max_iters:\n",
    "            print(f\"Max iterations reached: {max_iters}\")\n",
    "            break\n",
    "\n",
    "        w0_old, w_old = w0_new, w_new\n",
    "        \n",
    "        num_iters += 1\n",
    "\n",
    "    return w0_new, w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put everything together into a `MHCLogisticRegressor` class. Before we define the class, let's first take a look at a useful numpy function: `np.argmax()`.\n",
    "\n",
    "`np.argmax()` returns the **index** of the maximum value in an array. When used with `axis=1` on a 2D array, it returns the index of the maximum value in each row. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    proba = np.array([[0.8, 0.2],  # class 0 probability is 0.8\n",
    "                      [0.3, 0.7],  # class 1 probability is 0.7\n",
    "                      [0.6, 0.4]]) # class 0 probability is 0.6\n",
    "\n",
    "    # prints the index of the maximum value in each row\n",
    "    print(np.argmax(proba, axis=1)) # outputs: array([0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful for converting predicted probabilities into class predictions: if the class 0 probability (column 0) is higher, `argmax` returns 0; if the class 1 probability (column 1) is higher, it returns 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} ML model class documentation\n",
    "\n",
    "Like HW 1, please also ensure that you document your class and its methods.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from typing import Self\n",
    "\n",
    "class MHCLogisticRegressor(BaseEstimator):\n",
    "    \"\"\"TODO class description\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float, lam: float, max_iters: int = 5000):\n",
    "        \"\"\"TODO constructor description\"\"\"\n",
    "\n",
    "        # TODO initialize the self parameters\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> Self:\n",
    "        \"\"\"TODO method description\"\"\"\n",
    "\n",
    "        # TODO: call logreg_grad_descent to find the optimal weights\n",
    "        self.w0_, self.w_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"TODO method description\"\"\"\n",
    "\n",
    "        # TODO A (n,) shape array of class 1 predicted probabilities\n",
    "        class1_proba = None\n",
    "\n",
    "        # TODO A (n,) shape array of class 0 predicted probabilities\n",
    "        class0_proba = None\n",
    "\n",
    "        assert class0_proba.shape == class1_proba.shape, \"The class probabilities should have the same shape\"\n",
    "\n",
    "        # Stacks the class 0 and class 1 probabilities into a single array of shape (n, 2)\n",
    "        return np.column_stack([class0_proba, class1_proba])\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"TODO method description\"\"\"\n",
    "\n",
    "        # A (n,) shape array of predictions: use argmax with an axis reduction on the predicted probabilities array\n",
    "        predictions = None\n",
    "\n",
    "        assert predictions.shape == (X.shape[0],), \"The predictions should have shape (n,)\"\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Some linearly separable data\n",
    "    X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0],\n",
    "                  [6.0, 7.0], [7.0, 8.0], [8.0, 9.0]])\n",
    "    y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "    model = MHCLogisticRegressor(alpha=0.05, lam=0.0, max_iters=10000)\n",
    "    model = model.fit(X, y)\n",
    "\n",
    "    # Check predict_proba shape\n",
    "    proba = model.predict_proba(X)\n",
    "    assert proba.shape == (6, 2), f\"predict_proba should return shape (n, 2), got {proba.shape}\"\n",
    "\n",
    "    # Check that probabilities sum to 1\n",
    "    assert np.allclose(proba.sum(axis=1), 1.0), \"Probabilities should sum to 1 for each example\"\n",
    "\n",
    "    # Check predictions\n",
    "    preds = model.predict(X)\n",
    "    assert preds.shape == (6,), f\"predict should return shape (n,), got {preds.shape}\"\n",
    "    assert np.array_equal(preds, y), f\"Model should correctly classify linearly separable data, got {preds}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} How to submit\n",
    ":class: tip\n",
    "\n",
    "Follow the instructions on the [course website](https://comsc335.github.io/syllabus/submit.html) to submit your work. For part 1, you will submit `hw2_foundations.ipynb` and `hw2_foundations.py`.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
