{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "(hw2_predictions)=\n",
    "\n",
    "# HW 2 part 2\n",
    "\n",
    ":::{epigraph}\n",
    "Classification: Predictions\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-rubric",
   "metadata": {},
   "source": [
    "## Part 2 Table of Contents and Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|---|---|\n",
    "| Datasheets for Datasets | 1.5 |\n",
    "| Data Preparation | 1 |\n",
    "| Tuning and Prediction | 2 |\n",
    "| Reflection | 0.5 |\n",
    "| Total | 5 pts |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-imports-md",
   "metadata": {},
   "source": [
    "# Notebook and function imports\n",
    "\n",
    "If you have tested your implementation in Part 1 against the autograder, you would have generated a file called `hw2_foundations.py`. Let's now import those functions into this notebook for use in Part 2.\n",
    "\n",
    "If you are running this notebook on the JupyterHub allocated for the course:\n",
    "\n",
    "1. Open the file browser by going to the menu bar \"View -> File Browser\"\n",
    "2. Navigate to `comsc335.github.io/hws/`, you should see your `hw2_predictions.ipynb` file in that folder\n",
    "3. Click on the upload button in the upper right and upload the `hw2_foundations.py` file to this directory\n",
    "4. Run the following cell below to import the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6946c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:42.197823Z",
     "iopub.status.busy": "2026-02-22T04:18:42.197504Z",
     "iopub.status.idle": "2026-02-22T04:18:42.235026Z",
     "shell.execute_reply": "2026-02-22T04:18:42.234436Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:42.236807Z",
     "iopub.status.busy": "2026-02-22T04:18:42.236667Z",
     "iopub.status.idle": "2026-02-22T04:18:43.540025Z",
     "shell.execute_reply": "2026-02-22T04:18:43.539375Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import your implementations from Part 1\n",
    "from hw2_foundations import MHCLogisticRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sep1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd48b8c3",
   "metadata": {},
   "source": [
    ":::{admonition} Discussion questions\n",
    "\n",
    "Whenever a question asks for a discussion, we are not necessarily looking for a particular answer. However, we are looking for engagement with the material, so one-word/one-phrase answers usually don't give enough space to show your thought process. Try to explain your reasoning in ~1-2 full sentences.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q4-header",
   "metadata": {},
   "source": [
    "# 4. Datasheets for Datasets [1.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q4-preamble",
   "metadata": {},
   "source": [
    "As machine learning practitioners, we need to understand development process and intended purpose of the data we work with. Building off the idea that data is never \"neutral\" from last homework's readings, we will now look at the **Datasheets for Datasets** framework, proposed by [Timnit Gebru](https://www.dair-institute.org/team/timnit-gebru/) et al. in 2021, which provides a standardized set of questions for dataset documentation to help increase transparency and accountability in ML systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q41-header",
   "metadata": {},
   "source": [
    ":::{admonition} Gebru et al. 2021: Datasheets for Datasets\n",
    "\n",
    "Read pg 86 - 89 of the [Datasheets for Datasets paper](https://dl.acm.org/doi/pdf/10.1145/3458723), which covers through the **Motivation** and **Composition** questions of the datasheet. Then answer the questions below.\n",
    "\n",
    ":::\n",
    "\n",
    "**4.1**: What are the two reasons the authors give for why a model might perform poorly \"in the wild,\" even if it performs well on a benchmark?\n",
    "\n",
    "**4.2**: Describe the two key stakeholder groups and the **primary objectives** datasheets are designed to serve for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558aab9",
   "metadata": {},
   "source": [
    "**TODO your responses:**\n",
    "\n",
    "4.1:\n",
    "\n",
    "4.2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0622fec",
   "metadata": {},
   "source": [
    "Next, we'll take a closer look at the Adult Income dataset we began to work with in Worksheet 2.\n",
    "\n",
    ":::{admonition} Examining the context of the Adult Income dataset\n",
    "\n",
    "Watch the first 8:01 of the folktables paper presentation: [https://www.youtube.com/watch?v=KP7DhM_ahHI](https://www.youtube.com/watch?v=KP7DhM_ahHI). This video discusses how the folktables package was created as a replacement for the widely-used UCI Adult Income dataset.\n",
    "\n",
    "Look at the UCI Adult \"Dataset Information\" on the UCI Machine Learning Repository: [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult). This is the original documentation for the dataset created back in 1996 and is very sparse.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "Identify two specific questions from the Datasheets Motivation or Composition sections (questions 1 - 19) that the original UCI Adult dataset page does **not** answer. For each, briefly explain (~1-2 sentences) why knowing the answer would be important for someone using this dataset in an ML system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q41-response",
   "metadata": {},
   "source": [
    "**TODO your response:**\n",
    "\n",
    "4.3:\n",
    "\n",
    "- Question 1:\n",
    "\n",
    "- Question 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342615f",
   "metadata": {},
   "source": [
    ":::{admonition} A note on sex in this dataset\n",
    "\n",
    "The Adult (reconstructed) dataset, drawn from the 1994 US Census, records sex as a binary variable (`Male`/`Female`). While sex and gender are distinct concepts, both are more complex than a binary categorization captures.  In the fair machine learning literature, sex and race are commonly studied as **protected attributes**: characteristics that models should not use to discriminate against individuals.\n",
    "\n",
    "As ML practitioners, it is important to recognize that the categories in our data are shaped by the social and institutional contexts in which the data was collected. When we train models on data with binary sex categories, we build systems that cannot account for people who don't fit neatly into those categories. This is one example of how dataset design decisions carry forward into the models we build.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sep2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee53b",
   "metadata": {},
   "source": [
    "Now that we have some more context on the Adult Income dataset, let's prepare it for modeling. We'll practice the foundations of classification in this assignment and then explore fairness topics in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q5-header",
   "metadata": {},
   "source": [
    "# 5. Data Preparation [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q50-md",
   "metadata": {},
   "source": [
    "Before we can train a model, we need to prepare the data into a format our model can use. This involves:\n",
    "\n",
    "1. **Loading** the dataset and creating a binary target column\n",
    "2. **Encoding** categorical features as numeric columns (one-hot encoding)\n",
    "3. **Standardizing** numeric features so they are on a similar scale\n",
    "4. **Splitting** the data into training and test sets\n",
    "\n",
    "Let's start by loading the ACS Income (adult reconstructed) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q51-header",
   "metadata": {},
   "source": [
    "## 5.1 `prepare_data()` [0.5 pts]\n",
    "\n",
    "Below is a partial implementation of the `prepare_data()` function. Complete the data preparation steps to:\n",
    "\n",
    "1. Create a binary `income_>50k` column (1 if income >= 50000, 0 otherwise)\n",
    "2. One-hot encode the categorical columns using `pd.get_dummies()`: see [Worksheet 2](https://comsc335.github.io/worksheets/ws2.html#processing-categorical-variables-with-get-dummies) as a reference\n",
    "3. Drop non-feature columns from the dataframe using `drop()`: see [Worksheet 2](https://comsc335.github.io/worksheets/ws2.html#column-selection-and-filtering) as a reference\n",
    "\n",
    "The categorical columns in this dataset are: `workclass`, `education`, `marital-status`, `occupation`, `race`, `sex`, `native-country`.\n",
    "\n",
    "We drop the following columns: `income` and `relationship`.\n",
    "\n",
    ":::{tip} \n",
    "\n",
    "For both the `drop()` and `get_dummies()` methods, you can pass in the argument `columns=[]` to specify multiple columns to drop or encode in one go.\n",
    "\n",
    "To help with gradient descent convergence, we often **standardize** the features so they are on a similar scale. After standardizing, the numeric features will have a mean of 0 and a standard deviation of 1. We will cover standardization as part of our coverage of data preprocessing later in the course, so we provide the code to do so here. The `StandardScaler` object follows the same `fit()` and `transform()` interface as the `PolynomialFeatures` object we saw in Activity 7.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q51-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.609573Z",
     "iopub.status.busy": "2026-02-22T04:18:43.609460Z",
     "iopub.status.idle": "2026-02-22T04:18:43.640014Z",
     "shell.execute_reply": "2026-02-22T04:18:43.639422Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare the income dataframe for modeling.\n",
    "\n",
    "    Args:\n",
    "        df: raw income dataframe\n",
    "\n",
    "    Returns:\n",
    "        X: feature matrix as numpy array\n",
    "        y: target vector as numpy array\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid modifying the original\n",
    "    features_df = df.copy()\n",
    "\n",
    "    # TODO create the binary target column\n",
    "    features_df['income_>50k'] = None\n",
    "\n",
    "    # TODO drop non-feature columns. You can use the drop() method with a list of column names.\n",
    "    features_df = None\n",
    "\n",
    "    # TODO: One-hot encode categorical columns using pd.get_dummies(dtype=int) \n",
    "    categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex', 'native-country']\n",
    "    features_df = None\n",
    "\n",
    "    # Standardize numeric columns to be a similar range: mean 0, std 1\n",
    "    numeric_cols = ['age', 'hours-per-week', 'capital-gain', 'capital-loss', 'education-num']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features_df[numeric_cols])\n",
    "    features_df[numeric_cols] = scaler.transform(features_df[numeric_cols])\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q51-test",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.641715Z",
     "iopub.status.busy": "2026-02-22T04:18:43.641607Z",
     "iopub.status.idle": "2026-02-22T04:18:43.699460Z",
     "shell.execute_reply": "2026-02-22T04:18:43.698874Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test prepare_data\n",
    "    income_df = pd.read_csv('~/COMSC-335/data/adult_reconstruction_shuffled.csv')\n",
    "    income_features = prepare_data(income_df)\n",
    "    assert 'income_>50k' in income_features.columns, \"income_>50k column should be created\"\n",
    "    assert 'income' not in income_features.columns, \"income column should be dropped\"\n",
    "    assert income_features.shape == (49531, 102), f\"Expected feature_df shape (49531, 102), got {income_features.shape}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q52-header",
   "metadata": {},
   "source": [
    "## 5.2 `holdout_split()` [0.5 pts]\n",
    "\n",
    "Now we implement the holdout method by splitting our data into a **training set** (used to fit the model) and a **test set** (used to evaluate the model on unseen data). In practice, data should always be randomized (shuffled) before splitting into train and test sets. The CSV for this assignment has been pre-shuffled, so we don't need to shuffle it ourselves. Write a function that performs this split using the index slicing we practiced in [Activity 6](https://comsc335.github.io/activities/activity6_live.html#activity6-live). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q52-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.701047Z",
     "iopub.status.busy": "2026-02-22T04:18:43.700936Z",
     "iopub.status.idle": "2026-02-22T04:18:43.713205Z",
     "shell.execute_reply": "2026-02-22T04:18:43.712651Z"
    }
   },
   "outputs": [],
   "source": [
    "def holdout_split(features_df: pd.DataFrame, y_column: str, train_frac: float = 0.5) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into training and test sets via the holdout method.\n",
    "\n",
    "    Args:\n",
    "        features_df: pandas DataFrame of features\n",
    "        y_column: name of the target column\n",
    "        train_frac: fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (X_train, X_test, y_train, y_test) as numpy arrays\n",
    "    \"\"\"\n",
    "    # Compute the number of training examples\n",
    "    n_train = int(train_frac * features_df.shape[0])\n",
    "    \n",
    "    # TODO: split X and y into train and test sets using index slicing and n_train\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "\n",
    "    # TODO drop the y_column from X_train and X_test\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q52-test",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.715097Z",
     "iopub.status.busy": "2026-02-22T04:18:43.714963Z",
     "iopub.status.idle": "2026-02-22T04:18:43.725808Z",
     "shell.execute_reply": "2026-02-22T04:18:43.725296Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    income_df = pd.read_csv('~/COMSC-335/data/adult_reconstruction_shuffled.csv')\n",
    "    income_features = prepare_data(income_df)\n",
    "    # Test holdout_split with 50/50 split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = holdout_split(income_features, y_column='income_>50k', train_frac=0.5)\n",
    "    \n",
    "    assert X_train.shape[0] == 24765, \"Training set should have 24765 examples with a 50/50 split\"\n",
    "    assert X_test.shape[0] == 24766, \"Test set should have 24766 examples with a 50/50 split\"\n",
    "    assert X_train.shape[1] + 1 == income_features.shape[1], \"income_>50k column should be dropped from X_train\"\n",
    "    assert X_test.shape[1] + 1 == income_features.shape[1], \"income_>50k column should be dropped from X_test\"\n",
    "    assert y_train.shape[0] == X_train.shape[0], \"y_train should have the same number of examples as X_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q6-header",
   "metadata": {},
   "source": [
    "# 6. Prediction and Tuning [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q60-md",
   "metadata": {},
   "source": [
    "Now that our data is prepared, let's now follow the ML process to train and evaluate our logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q61-header",
   "metadata": {},
   "source": [
    "## 6.1 Accuracy [0.5 pts]\n",
    "\n",
    "Write a function that computes classification accuracy, which is the fraction of predictions that match the true labels:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i = \\hat{y}_i)\n",
    "$$\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This expression can be translated almost directly into code by using numpy array boolean indexing and `np.mean()`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q61-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.727386Z",
     "iopub.status.busy": "2026-02-22T04:18:43.727280Z",
     "iopub.status.idle": "2026-02-22T04:18:43.738406Z",
     "shell.execute_reply": "2026-02-22T04:18:43.737866Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"Compute classification accuracy.\n",
    "\n",
    "    Args:\n",
    "        y_pred: predicted labels of shape (n,)\n",
    "        y_true: true labels of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        accuracy as a float\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q61-test",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.739945Z",
     "iopub.status.busy": "2026-02-22T04:18:43.739840Z",
     "iopub.status.idle": "2026-02-22T04:18:43.751432Z",
     "shell.execute_reply": "2026-02-22T04:18:43.750941Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test compute_accuracy\n",
    "    assert compute_accuracy(np.array([1, 0, 1, 1]), np.array([1, 0, 0, 1])) == 0.75, \"compute_accuracy() should return 0.75 for this example\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q62-header",
   "metadata": {},
   "source": [
    "## 6.2 $\\lambda$ sweep and the ML process [0.75 pts]\n",
    "\n",
    "The cell below contains the entire ML process:\n",
    "\n",
    "$$\n",
    "\\text{Data} \\rightarrow \\text{Features} \\rightarrow \\text{Model} \\rightarrow \\text{Train} \\rightarrow \\text{Evaluate}\n",
    "$$\n",
    "\n",
    "For our training process, we saw in [Activity 7](https://comsc335.github.io/activities/activity7_live.html#activity7-live) that regularization helps prevent overfitting and practiced finding a good value for the L2 regularization hyperparameter. Let's tune the `lam` hyperparameter for our logistic regression model by trying several values and picking the one with the best test accuracy. We'll try out the following values using `np.logspace(-5, 0, 6)`:\n",
    "\n",
    "$$\n",
    "\\lambda \\in \\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\}\n",
    "$$\n",
    "\n",
    "Complete the code below to train and evaluate a model for each value of `lam`. Keep the following hyperparameters for your `MHCLogisticRegressor` models constant:\n",
    "\n",
    "- `alpha=0.5`\n",
    "- `max_iters=2000`\n",
    "\n",
    ":::{admonition} Runtime note\n",
    ":class: note\n",
    "\n",
    "Each model may take 10-20 seconds to train depending on JupyterHub available resources.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q62-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:43.752920Z",
     "iopub.status.busy": "2026-02-22T04:18:43.752817Z",
     "iopub.status.idle": "2026-02-22T04:18:55.666667Z",
     "shell.execute_reply": "2026-02-22T04:18:55.665975Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw2_foundations import MHCLogisticRegressor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Data: load in the raw data\n",
    "    income_df = pd.read_csv('~/COMSC-335/data/adult_reconstruction_shuffled.csv')\n",
    "\n",
    "    # TODO Features: call your prepare_data() function to featurize your data\n",
    "    income_features = None\n",
    "\n",
    "    # TODO Data/Features: call your holdout_split() function to split your data into train and test sets\n",
    "    # y_column='income_>50k', and train_frac=0.5\n",
    "    X_train, X_test, y_train, y_test = None\n",
    "\n",
    "    # We'll tune the lambda hyperparameter for our model\n",
    "    lams = np.logspace(-5, 0, 6)\n",
    "\n",
    "    \n",
    "    best_acc = 0\n",
    "    # Save the best model to analyze later\n",
    "    best_model = None\n",
    "\n",
    "    for lam in lams:\n",
    "        # TODO Model: initialize your MHCLogisticRegressor model from part 1 \n",
    "        # Use: alpha=0.5, lam=lam, max_iters=2000\n",
    "        model = None\n",
    "\n",
    "        # TODO Train: call your model's fit() method to train on the training data\n",
    "\n",
    "        # TODO Evaluate: compute train and test accuracy using your compute_accuracy() function\n",
    "        train_acc = 0\n",
    "        test_acc = 0\n",
    "\n",
    "        # if the current model has the highest test accuracy so far, save it\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"  lambda={lam:.5f} | Train accuracy: {train_acc:.4f} | Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a211a0b",
   "metadata": {},
   "source": [
    "Briefly discuss the results of your lambda sweep and report the best value of lambda. What value of lambda gave the best test accuracy? Was this best value clearly better than the other values of lambda, or was there a range of values that performed similarly?\n",
    "\n",
    "**Your response:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q62-solution",
   "metadata": {},
   "source": [
    ":::{admonition} Click to check accuracy results\n",
    ":class: dropdown\n",
    "\n",
    "Depending on your `MHCLogisticRegressor` implementation and the available resources on JupyterHub, you should see a test accuracy in the range of 0.82-0.86 for the best values of lambda.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q63-header",
   "metadata": {},
   "source": [
    "## 6.3 Error analysis [0.75 pts]\n",
    "\n",
    "A model's overall accuracy only tells part of the story. To understand **how** a model is making mistakes, it is useful to examine the examples it gets wrong. In binary classification, there are two types of errors:\n",
    "\n",
    "- **False positives**: the model predicts income $>$ \\$50k, but the true label is $\\leq$ \\$50k\n",
    "- **False negatives**: the model predicts income $\\leq$ \\$50k, but the true label is > \\$50k\n",
    "\n",
    "Let's use the pandas boolean indexing operations we practiced in Worksheet 2 to examine these errors. Below is starter code to perform this analysis. We save the predictions from the best model in `best_model` to the \"test\" portion of the `income_df`. It then creates a `test_df` dataframe that contains the test set examples and the following columns:\n",
    "\n",
    "- `predicted`: the model's predicted label (1 if income > 50k, 0 otherwise)\n",
    "- `income_>50k`: the true label (1 if income > 50k, 0 otherwise)\n",
    "\n",
    "Complete the TODO below to create the `false_pos` and `false_neg` columns. You may need to re-run the cell above to make sure that the `best_model` variable is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-q63-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T04:18:55.668418Z",
     "iopub.status.busy": "2026-02-22T04:18:55.668292Z",
     "iopub.status.idle": "2026-02-22T04:18:55.688324Z",
     "shell.execute_reply": "2026-02-22T04:18:55.687680Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    income_df = pd.read_csv('~/COMSC-335/data/adult_reconstruction_shuffled.csv')\n",
    "\n",
    "    # Convert income to binary target: 1 if income > 50k, 0 otherwise\n",
    "    income_df['income_>50k'] = (income_df['income'] > 50000).astype(int)\n",
    "\n",
    "    # Create the test portion of the dataframe based on the shape of the training set\n",
    "    n_train = X_train.shape[0]\n",
    "    test_df = income_df[n_train:].copy()\n",
    "\n",
    "    # Save predictions from the best model to the \"test\" portion of the income_df dataframe into the \"predicted\" column\n",
    "    # predicted = 1 if the model predicts income > 50k, 0 otherwise\n",
    "    test_df['predicted'] = best_model.predict(X_test)\n",
    "\n",
    "    # TODO create false_pos and false_neg columns\n",
    "    # The \"predicted\" column contains the model's predictions, and the \"income_>50k\" column contains the true labels.\n",
    "    # Make sure to wrap your boolean expressions in parentheses for correct evaluation.\n",
    "    test_df['false_pos'] = None\n",
    "    test_df['false_neg'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # test that false_pos and false_neg are correct\n",
    "    false_pos_df = test_df[test_df['false_pos'] == 1]\n",
    "    false_neg_df = test_df[test_df['false_neg'] == 1]\n",
    "    assert np.all(false_pos_df['predicted'] == 1), \"false_pos_df should only contain examples where the model predicted 1\"\n",
    "    assert np.all(false_pos_df['income_>50k'] == 0), \"false_pos_df should only contain examples where the true label is 0\"\n",
    "    assert np.all(false_neg_df['predicted'] == 0), \"false_neg_df should only contain examples where the model predicted 0\"\n",
    "    assert np.all(false_neg_df['income_>50k'] == 1), \"false_neg_df should only contain examples where the true label is 1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07618dca",
   "metadata": {},
   "source": [
    "Now, pick one of the categorical features below to examine in the widget below:\n",
    "- `sex`\n",
    "- `race`\n",
    "- `education`\n",
    "- `occupation`\n",
    "- `workclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys; sys.path.insert(0, '..')\n",
    "    from utils import explore_categorical_errors\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "    categorical_cols = ['workclass', 'education', 'occupation', 'race', 'sex']\n",
    "    error_types = ['False Positive Rate (%)', 'False Negative Rate (%)']\n",
    "\n",
    "    widgets.interact(explore_categorical_errors, \n",
    "        # Tells the widget to use the test_df dataframe you created above\n",
    "        df=widgets.fixed(test_df), \n",
    "        # Creates a dropdown for the column name\n",
    "        column_name=categorical_cols,\n",
    "        # Creates a dropdown for the error type\n",
    "        error_type=error_types\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q63a-md",
   "metadata": {},
   "source": [
    "Use the widget to look at the **false positive rate** and **false negative rate** for each category within your chosen feature:\n",
    "\n",
    "- The false positive rate is the percentage of actually <\\$50k income individuals that the model incorrectly predicts as >\\$50k. \n",
    "- The false negative rate is the percentage of actually >\\$50k income individuals that the model incorrectly predicts as <\\$50k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-q63-reflection-md",
   "metadata": {},
   "source": [
    "6.3.1: For both the false positive rate and the false negative rate, compare that rate across the categories of your chosen feature. Are the rates similar across categories, or are there notable differences? Which categories have the highest rate, and which have the lowest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa86ff",
   "metadata": {},
   "source": [
    "**Your response:**\n",
    "\n",
    "- Chosen feature: TODO\n",
    "\n",
    "- False positive rate: TODO\n",
    "\n",
    "- False negative rate: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf08f0",
   "metadata": {},
   "source": [
    "**6.3.2**: Suppose this model was used to make a real-world decision: approving loan applications based on an income eligibility cutoff, where individuals with income above \\$50k are more likely to be approved and those below are more likely to be rejected.\n",
    "\n",
    "Briefly discuss what the differences you observed above might mean for individuals' chances of loan approval in the affected categories (~2-3 sentences). It's okay to speculate here as long as you provide some rationale, as this is an open-ended question. \n",
    "\n",
    ":::{note} \n",
    "\n",
    "You may notice that the types of errors the model makes are not evenly distributed across groups of individuals. In the upcoming weeks, weâ€™ll discuss ML fairness and evaluation tools for investigating these kinds of disparities, interpreting them in context, and considering the real-world impacts of ML-assisted decision-making.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a9c50",
   "metadata": {},
   "source": [
    "**Your response:**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sep3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection",
   "metadata": {},
   "source": [
    "# 7. Reflection [0.5 pts]\n",
    "\n",
    "1. How much time did you spend on this assignment?\n",
    "\n",
    "2. Were there any parts of the assignment that you found particularly challenging?\n",
    "\n",
    "3. What is one thing you have a better understanding of after completing this assignment and going through the class content?\n",
    "\n",
    "4. Do you have any follow-up questions about concepts that you'd like to explore further?\n",
    "\n",
    "5. Indicate the number of late days (if any) you are using for this assignment.\n",
    "\n",
    "**TODO** your responses here:\n",
    "\n",
    "7.1:\n",
    "\n",
    "7.2:\n",
    "\n",
    "7.3:\n",
    "\n",
    "7.4:\n",
    "\n",
    "7.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-submit",
   "metadata": {},
   "source": [
    ":::{admonition} How to submit\n",
    ":class: tip\n",
    "\n",
    "Follow the instructions on the [course website](https://comsc335.github.io/syllabus/submit.html) to submit your work. For part 2, you will submit `hw2_predictions.ipynb` and `hw2_predictions.py`.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
