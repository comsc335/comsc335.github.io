{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "(activity7)=\n",
    "\n",
    "# Activity 7: Polynomial Features and Ridge Regression\n",
    "\n",
    "**2026-02-17**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Root mean squared error.\"\"\"\n",
    "    assert y_hat.shape == y.shape\n",
    "    return np.sqrt(np.mean((y_hat - y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5a6b7",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "We'll use the same California housing dataset and 80/20 split from Activity 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split (same as Activity 6)\n",
    "housing_df = pd.read_csv(\"~/COMSC-335/data/housing_data.csv\")\n",
    "\n",
    "housing_train = housing_df[:4000]\n",
    "housing_test = housing_df[4000:]\n",
    "\n",
    "X_train = housing_train.drop(columns=[\"MedHouseVal\"])\n",
    "X_test = housing_test.drop(columns=[\"MedHouseVal\"])\n",
    "\n",
    "# apply standardization to all features, helps with numerical stability\n",
    "# we'll talk more preprocessing in future lectures!\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = housing_train[\"MedHouseVal\"].to_numpy()\n",
    "y_test = housing_test[\"MedHouseVal\"].to_numpy()\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} examples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} examples, {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa857b9c",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a79e39",
   "metadata": {},
   "source": [
    "`PolynomialFeatures` is what is known as a \"transformer\" in scikit-learn (not the neural network kind). The standard workflow is:\n",
    "\n",
    "- Initialize the transformer object with any hyperparameters\n",
    "- Call `fit()` on the training data: if the transformer has parameters, it will learn them from the training data\n",
    "- Call `transform()` on both the training and test data: use the learned parameters to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a degree 2 polynomial feature from a single feature $x$\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit the transformer to the training data\n",
    "poly_features.fit(X_train)\n",
    "\n",
    "# transform both the training and test data\n",
    "X_poly_train = poly_features.transform(X_train)\n",
    "X_poly_test = poly_features.transform(X_test)\n",
    "\n",
    "# Examine the shape of the transformed data\n",
    "print(X_poly_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d00d4f",
   "metadata": {},
   "source": [
    "We'll combine polynomial features with a ridge-regularized linear regression model: `Ridge(alpha=a)`.\n",
    "\n",
    "The `Ridge` model operates exactly like all of our other scikit-learn models, with a `fit()` method and a `predict()` method.\n",
    "\n",
    "We need to set the regularization hyperparameter `alpha` (called $\\lambda$ in the lecture slides and in most other ML literature) to a positive value if we want to use regularization.\n",
    "\n",
    "If we set `alpha=0`, we get back the unregularized linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Ridge model with alpha = 1.0\n",
    "ridge_model = Ridge(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4e891",
   "metadata": {},
   "source": [
    "Let's see what the largest weights are with and without ridge regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 3\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=deg)\n",
    "\n",
    "# fit the transformer to the training data\n",
    "poly_features.fit(X_train)\n",
    "\n",
    "X_poly_train = poly_features.transform(X_train)\n",
    "X_poly_test = poly_features.transform(X_test)\n",
    "\n",
    "print(\"Degree\", deg, \"polynomial features:\", X_poly_train.shape[1])\n",
    "\n",
    "ridge_model = Ridge(alpha=0)\n",
    "\n",
    "ridge_model.fit(X_poly_train, y_train)\n",
    "\n",
    "# we can access the fitted weights using the `coef_` attribute\n",
    "print(ridge_model.coef_)\n",
    "\n",
    "# np.abs() returns the absolute value of each element in the array\n",
    "# np.max() returns the maximum value in the array   \n",
    "np.max(np.abs(ridge_model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d0bc8",
   "metadata": {},
   "source": [
    "By convention, we typically try out regularization hyperparameters in the range of powers of 10, e.g:\n",
    "\n",
    "$$\n",
    "[10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2, 10^3]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a5ce6",
   "metadata": {},
   "source": [
    "There is a convenient [np.logspace()](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html) function that generates values in this range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac017c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb166456",
   "metadata": {},
   "source": [
    "Below is starter code for training a ridge model and evaluating its train and test RMSE. Complete the code to:\n",
    "\n",
    "- Transform the training and test data using `PolynomialFeatures`\n",
    "- Iterate over the range of alphas\n",
    "\n",
    "Experiment with higher values of the degree. Compare with folks around you what values of the regularization hyperparameter and degree give the best test RMSE, and submit your best test RMSE to PollEverywhere:\n",
    "\n",
    "https://pollev.com/tliu\n",
    "\n",
    "Discuss with folks around you: from what you're seeing, is a higher or lower `alpha` needed to get most of the benefits of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e28383",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "poly_features = None # TODO initialize the transformer\n",
    "\n",
    "# TODO fit the transformer on the training data\n",
    "\n",
    "\n",
    "# TODO transform both the training and test data\n",
    "X_poly_train = None\n",
    "X_poly_test = None\n",
    "\n",
    "print(\"Degree\", degree, \"polynomial features:\", X_poly_train.shape[1])\n",
    "\n",
    "# TODO iterate over alphas here\n",
    "alphas = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    # TODO initialize the model with the current alpha\n",
    "    ridge_model = None\n",
    "\n",
    "    # TODO fit the model on the polynomial training data\n",
    "    \n",
    "\n",
    "    # TODO compute the train and test RMSE\n",
    "    train_rmse = rmse(ridge_model.predict(X_poly_train), y_train)\n",
    "    test_rmse = rmse(ridge_model.predict(X_poly_test), y_test)\n",
    "\n",
    "    print(f'  alpha={alpha:.4f} | Train: {train_rmse:.4f} | Test: {test_rmse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
