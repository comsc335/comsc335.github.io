{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbbc4c9",
   "metadata": {},
   "source": [
    "(hw1_foundations)=\n",
    "\n",
    "# HW 1 part 1\n",
    "\n",
    ":::{epigraph}\n",
    "Linear Regression: Foundations\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa2ef8",
   "metadata": {},
   "source": [
    "## Part 1 Table of Contents and Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|------------------------------------|-------|\n",
    "| Mathematical Foundations | 2 |\n",
    "| Gradient Descent Implementation | 2.5 |\n",
    "| Total | 4.5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f35fdc",
   "metadata": {},
   "source": [
    "# Notebook and function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# random number generator\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e24b7",
   "metadata": {},
   "source": [
    "# 1. Mathematical Foundations [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc301d1",
   "metadata": {},
   "source": [
    "We'll begin by deriving the equations that we'll need to build our linear regression model. Building off what we did in class, linear regression can be expanded to include multiple features.\n",
    "\n",
    "In particular, if we have a dataset with $n$ datapoints and $p$ features, then the linear regression model can be written as:\n",
    "\n",
    "$$\n",
    "f(\\vec{x_i}) = w_0 + w_1 x_{i,1} + w_2 x_{i,2} + \\cdots + w_p x_{i,p}\n",
    "$$\n",
    "\n",
    "Where $\\vec{x_i} = (x_{i,1}, x_{i,2}, \\ldots, x_{i,p})$ is a vector of features.\n",
    "\n",
    "We then pair this model with the **mean squared error** loss function that measures the squared error between the predicted and true values:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{w}) = \\frac{1}{n} \\sum_{i=1}^n (f(\\vec{x_i}) - y_i)^2\n",
    "$$\n",
    "\n",
    "The question we now have is: how do we find the \"best\" values for the weights $\\vec{w} = (w_0, w_1, \\ldots, w_p)$ so that the loss function is minimized? To get us started, we'll need the partial derivatives of the loss function with respect to the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a93553",
   "metadata": {},
   "source": [
    "## 1.1 Linear regression gradients [1 pt] \n",
    "\n",
    "Compute the partial derivative of the loss function $\\mathcal{L}(\\vec{w})$ with respect to the weight $w_0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_0} =  \\frac{\\partial}{\\partial w_0} \\frac{1}{n} \\sum_{i=1}^n (f(\\vec{x}_i)- y_i)^2\n",
    "$$\n",
    "\n",
    "Your response:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_0} = \\text{ TODO }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e545c0a",
   "metadata": {},
   "source": [
    "Now, compute the partial derivative of the loss function with respect to the weight $w_j$ where $j \\in \\{1, 2, 3,\\ldots, p\\}$, showing your work:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} =  \\frac{\\partial}{\\partial w_j} \\frac{1}{n} \\sum_{i=1}^n (f(\\vec{x}_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Your response:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\text{ TODO } \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d26be",
   "metadata": {},
   "source": [
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This derivation should closely follow what we did in class, except now generalized to $p$ features. Please make an initial attempt on your own but feel free to look at the slides if you get stuck.\n",
    "\n",
    "Remember that when computing partial derivatives, we treat all variables except the one we are differentiating as constants!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13229aae",
   "metadata": {},
   "source": [
    "## 1.2 Gradient descent constant factors [0.5 pts]\n",
    "\n",
    "\n",
    "In certain textbooks, you might see the loss function for linear regression written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{w}) = \\frac{1}{2n} \\sum_{i=1}^n (f(\\vec{x}_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "where there is an extra constant factor of $\\frac{1}{2}$ in the loss function. Does multiplying the loss function by a constant factor change which weights are optimal (i.e., minimize the loss)?\n",
    "\n",
    "\n",
    "To help us answer this question, let's complete a small derivation. Suppose we have a function $h(w)$. Let $w^*$ be the value of $w$ that **minimizes** $h(w)$ over the domain of all real numbers $\\mathbb{R}$. That means for all other values of $w \\in \\mathbb{R}$, we have that $h(w^*) \\leq h(w)$. Formally, we can write this as:\n",
    "\n",
    "$$\n",
    "\\forall w \\in \\mathbb{R}, \\quad h(w^*) \\leq h(w)\n",
    "$$\n",
    "\n",
    "Next, let $g(w) = c \\cdot h(w)$, where $c > 0$ is a constant. Show that $w^*$ also minimizes $g(w)$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\forall w \\in \\mathbb{R}, \\quad h(w^*) &\\leq h(w) \\\\\n",
    "                                       &\\leq \\text{ TODO your step 1 } \\\\\n",
    "                                       &\\leq \\text{ TODO your step 2 }\n",
    "$$\n",
    "\n",
    "Note that this same idea works when the input to the function is a vector of weights $\\vec{w} = (w_0, w_1, \\ldots, w_p)$. \n",
    "\n",
    "Explain what this tells us about the weights that minimize the loss function $\\frac{1}{2n} \\sum_{i=1}^n (f(\\vec{x}_i) - y_i)^2$ versus the weights that minimize the loss function $\\frac{1}{n} \\sum_{i=1}^n (f(\\vec{x}_i) - y_i)^2$. Discuss why you think the loss function is sometimes written with the $\\frac{1}{2}$.\n",
    "\n",
    "**Your response:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e14d16",
   "metadata": {},
   "source": [
    ":::{admonition} Discussion questions\n",
    "\n",
    "Whenever a question asks for a discussion, we are not necessarily looking for a particular answer. However, we are looking for engagement with the material, so one-word/one-phrase answers usually don't give enough space to show your thought process. Try to explain your reasoning in ~1-2 full sentences.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a1d78",
   "metadata": {},
   "source": [
    "## 1.3 Contour plots [0.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6da026",
   "metadata": {},
   "source": [
    "![](figures/hw1_contour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997afde",
   "metadata": {},
   "source": [
    "Suppose we're optimizing the MSE loss function $\\mathcal{L}$ where there are two weights $w_1$ and $w_2$. A contour plot using this loss function is shown above. We can think of the partial derivatives $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ and $\\frac{\\partial \\mathcal{L}}{\\partial w_2}$ as vectors that point in the steepest direction of the loss function. \n",
    "\n",
    "To get us oriented with this contour plot: the darker the color, the lower the MSE loss, and some of the contour lines are also labelled with the loss value. For example, the labelled point $(w_1, w_2) = (-3, 2)$ sits on the contour with an MSE loss of around 3. \n",
    "\n",
    "\n",
    "**1.3.1:** Pick a point on the contour plot that has a **higher** loss than 3:\n",
    "\n",
    "$$\n",
    "(w_1, w_2) = \\text{ TODO your response}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The way we think about moving along the contour plot is that increasing or decreasing $w_1$ corresponds to moving vertically (up or down) along the contour plot, while increasing or decreasing $w_2$ corresponds to moving horizontally (left or right). Now, consider that the gradient descent algorithm updates the weights by moving in the direction of the **negative** partial derivatives to minimize the loss function. For example, the update rule for $w_1$ is given by:\n",
    "\n",
    "$$\n",
    "w_{1, \\text{new}} = w_{1, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_{1, \\text{old}}}\n",
    "$$\n",
    "\n",
    "for some learning rate $\\alpha$, and $w_{1, \\text{new}}$ will update such that the overall MSE loss is lower (subject to an appropriately chosen $\\alpha$). \n",
    "\n",
    "Now, consider the labelled point $(w_1, w_2) = (-3, 2)$:\n",
    "\n",
    "**1.3.2:** Does $w_1$ need to increase or decrease to move towards the minimum loss?\n",
    "\n",
    "**Your response:** TODO\n",
    "\n",
    "**1.3.3:** Does $w_2$ need to increase or decrease to move towards the minimum loss?\n",
    "\n",
    "**Your response:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a26615",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent Implementation [2.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f229f",
   "metadata": {},
   "source": [
    "## 2.1 Gradient update rule [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb496a81",
   "metadata": {},
   "source": [
    "For the rest of the homework, let's suppose that $p=2$, so we have two features $x_1$ and $x_2$. Then the linear regression model can be written as:\n",
    "\n",
    "$$\n",
    "f(\\vec{x_i}) = w_0 + w_1 x_{i,1} + w_2 x_{i,2}\n",
    "$$\n",
    "\n",
    "We can then implement the gradient update rule for the weights. Gradient descent updates the weights by moving in the direction of the negative partial derivatives, scaled by a learning rate $\\alpha$:\n",
    "\n",
    "$$\n",
    "w_{0, \\text{new}} &= w_{0, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_0} \\\\\n",
    "w_{1, \\text{new}} &= w_{1, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_1} \\\\\n",
    "w_{2, \\text{new}} &= w_{2, \\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_2}\n",
    "$$\n",
    "\n",
    "Implement this gradient update rule in `linreg_grad_update` below.\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This function can be written without any loops by using the slice notation for numpy arrays we practiced in Worksheet 1. You can take advantage of numpy's vectorized operations when computing the predictions for all the data points at once, which is much more efficient than computing the predictions one data point at a time.\n",
    "\n",
    "For example, `X[:, 0]` is a 1D numpy array of shape `(n,)` that contains the first feature of all the data points, and can be multiplied with `w_old[1]` to compute $w_1 x_{1}$ for all the data points simultaneously.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_grad_update(w_old: np.ndarray, X: np.ndarray, y: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    \"\"\"Perform a single gradient update for linear regression.\n",
    "\n",
    "    Args:\n",
    "        w_old: the old weights of shape (3,)\n",
    "        X: the feature matrix of shape (n, 2)\n",
    "        y: the target vector of shape (n,)\n",
    "        alpha: the learning rate\n",
    "\n",
    "    Returns:\n",
    "        The new weights of shape (3,)\n",
    "    \"\"\"\n",
    "\n",
    "    assert w_old.shape == (3,)\n",
    "\n",
    "    # TODO your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8b6d5",
   "metadata": {},
   "source": [
    ":::{tip} \n",
    "\n",
    "The simple test cases we provide via assert statements here usually have data that can be verified by hand if you would like to double check your implementation. \n",
    "\n",
    "You are encouraged to write your own test cases as well! If you do write additional test cases, just be sure that the assert statements are written within the `if __name__ == \"__main__\":` block so that the autograder can run correctly.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize some simple data for testing, n=3, p=2\n",
    "    X = np.array([[1, 2], \n",
    "                  [2, 3], \n",
    "                  [3, 3]])\n",
    "    y = np.array([0, 1, 2])\n",
    "    w_old = np.array([0, 0, 0])\n",
    "    alpha = 0.5\n",
    "\n",
    "    # Test the gradient update rule\n",
    "    w_new = linreg_grad_update(w_old, X, y, alpha)\n",
    "    assert w_new.shape == (3,), \"The new weights should have shape (3,)\"\n",
    "    assert np.allclose(w_new, np.array([1, 8/3, 3])), \"The new weights should be [1, 8/3, 3]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49947f",
   "metadata": {},
   "source": [
    ":::{admonition} Gradient notation\n",
    "As an aside, gradient update rule can be more compactly written in terms of the gradient $\\nabla \\mathcal{L}(\\vec{w})$, which is the vector of partial derivatives:\n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L}(\\vec{w}) = \\left(\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_0},\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_1},\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_2} \\right)\n",
    "$$\n",
    "\n",
    "The gradient update rule is then given by:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{\\text{new}} = \\vec{w}_{\\text{old}} - \\alpha \\nabla \\mathcal{L}(\\vec{w}_{\\text{old}})\n",
    "$$\n",
    "\n",
    "Where $\\vec{w}_{\\text{new}}$ and $\\vec{w}_{\\text{old}}$ are vectors of shape `(3,)` holding the new and old weights, respectively. We will discuss this representation in more detail in upcoming classes.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece295ed",
   "metadata": {},
   "source": [
    "## 2.2 MSE loss [0.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b5ed1",
   "metadata": {},
   "source": [
    "Now, let's implement the loss function. Recall from class that the loss function quantifies the amount of error that our model makes on the training data. Thus, our goal is to build a model that *minimizes* the loss \n",
    "function. From above, the loss function is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (f(\\vec{x}_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Our model $f(\\vec{x}_i)$ outputs predictions $\\hat{y}_i$, giving the loss function as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6040cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Compute the mean squared error loss for linear regression.\n",
    "\n",
    "    Args:\n",
    "        y_hat: the predicted values of shape (n,)\n",
    "        y: the target values of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        The mean squared error loss\n",
    "    \"\"\"\n",
    "    assert y_hat.shape == y.shape, \"y-hat predictions and y targets must have the same shape\"\n",
    "    \n",
    "    # TODO your code here\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d579d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test the loss function\n",
    "    y_hat = np.array([0, 1, 2])\n",
    "    y = np.array([0, 1, 2])\n",
    "    assert np.allclose(mse_loss(y_hat, y), 0), \"The loss should be 0\"\n",
    "\n",
    "    y_hat = np.array([1, 2, 4])\n",
    "    y = np.array([0, 1, 2])\n",
    "    assert np.allclose(mse_loss(y_hat, y), 2), \"The loss should be 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab6db1",
   "metadata": {},
   "source": [
    "## 2.3 Gradient descent [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aad311",
   "metadata": {},
   "source": [
    "We'll now implement the overall gradient descent algorithm. The algorithm begins by initializing the weights to arbitrary values, and then iteratively updates the weights until a stopping criterion is met. There are two stopping criteria:\n",
    "\n",
    "1. We stop if the weights have sufficiently **converged** to a minimum. Here, we test for convergence by checking if the Euclidean distance between the old and new weights is less than some small stopping threshold $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\left\\| \\vec{w}_{\\text{new}} - \\vec{w}_{\\text{old}} \\right\\|_2 < \\epsilon\n",
    "$$\n",
    "\n",
    "2. We also stop if the number of iterations exceeds a maximum number of iterations.\n",
    "\n",
    "The algorithm can be described in the following pseudocode:\n",
    "\n",
    "```python\n",
    "# Initialize the weights to arbitrary values\n",
    "w_new = [1e4, 1e4, 1e4]\n",
    "w_old = np.random.randn(3)\n",
    "while True:\n",
    "    # update the new weights\n",
    "    w_new = linreg_grad_update(w_old, X, y, alpha)\n",
    "\n",
    "    # check for convergence using Euclidean distance from Worksheet 1, exit the while loop if the weights have converged\n",
    "    \n",
    "    # check if number of iterations exceeds max_iters, exit the while loop if it does\n",
    "\n",
    "    # update the old weights\n",
    "    w_old = w_new\n",
    "```\n",
    "\n",
    "In addition to finding the final weights, your implementation will also return a list of the loss values and a list of the weights, appended at each iteration.\n",
    "\n",
    ":::{tip}\n",
    "\n",
    "In Python, you can use the `break` statement to exit the while loop early.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe66653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_grad_descent(X: np.ndarray, y: np.ndarray, alpha: float, stopping_threshold: float=1e-6, max_iters: int=10000) -> tuple[np.ndarray, list[float], list[np.ndarray]]:\n",
    "    \"\"\"Perform gradient descent for linear regression.\n",
    "\n",
    "    Additionally computes the loss function at each iteration.\n",
    "\n",
    "    Args:\n",
    "        X: the feature matrix of shape (n, 2)\n",
    "        y: the target vector of shape (n,)\n",
    "        alpha: the learning rate\n",
    "        stopping_threshold: the stopping threshold, default is 1e-6\n",
    "        max_iters: the maximum number of iterations, default is 10000\n",
    "\n",
    "    Returns:\n",
    "        Final weights of shape (3,)\n",
    "        A list of the loss function values at each iteration\n",
    "        A list of the weights at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the weights to arbitrary values\n",
    "    w_new = [1e4, 1e4, 1e4]\n",
    "    w_old = np.random.randn(3)\n",
    "\n",
    "    # Initialize the loss values list\n",
    "    loss_values = []\n",
    "\n",
    "    # Initialize the weight history list\n",
    "    w_history = []\n",
    "\n",
    "    # number of iterations\n",
    "    num_iters = 0\n",
    "\n",
    "    # TODO Complete the while loop\n",
    "    while True:\n",
    "        # Compute the loss function using the old weights\n",
    "\n",
    "    \n",
    "        # Append to the loss values list\n",
    "        \n",
    "\n",
    "        # Append the weights to the history\n",
    "        \n",
    "\n",
    "        # Update the new weights\n",
    "        \n",
    "\n",
    "        # Check for convergence, exit the loop if so\n",
    "        \n",
    "\n",
    "        # Check if number of iterations exceeds max_iters, exit the loop if so\n",
    "        \n",
    "        \n",
    "        # Update the old weights\n",
    "\n",
    "\n",
    "        # Increment the number of iterations\n",
    "        num_iters += 1\n",
    "        \n",
    "        \n",
    "    return w_new, loss_values, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72dce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize some simple data for testing, n=3, p=2\n",
    "    X = np.array([[1, 2], \n",
    "                  [2, 3], \n",
    "                  [3, 3]])\n",
    "    y = np.array([0, 1, 2])\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Test the gradient descent algorithm\n",
    "    w_final, loss_values, w_history = linreg_grad_descent(X, y, alpha)\n",
    "\n",
    "    assert w_final.shape == (3,), \"The final weights should have shape (3,)\"\n",
    "    assert loss_values[-1] < 1e-4, \"The final loss should be less than 1e-4\"\n",
    "    assert np.allclose(w_final, np.array([-1, 1, 0]), atol=1e-3), \"The final weights should be [-1, 1, 0]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420dbd8",
   "metadata": {},
   "source": [
    ":::{admonition} How to submit\n",
    ":class: tip\n",
    "\n",
    "\n",
    "Like Worksheet 1, follow the instructions on the [course website](https://comsc335.github.io/syllabus/submit.html) to submit your work. For part 1, you will submit `hw1_foundations.ipynb` and `hw1_foundations.py`.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
