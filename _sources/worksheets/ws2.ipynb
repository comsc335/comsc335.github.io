{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12c76eb",
   "metadata": {},
   "source": [
    "(ws2)=\n",
    "# Worksheet 2\n",
    "\n",
    ":::{epigraph}\n",
    "Datasets and Probability\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cee72",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Learn about `pandas` and `seaborn` for dataset manipulation and visualization.\n",
    "- Practice with probability concepts needed for the course:\n",
    "    - Discrete random events and expectation\n",
    "    - Contingency tables and conditional probabilities\n",
    "- Familiarization with broadcasting and axis operations in `numpy`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e1eaf",
   "metadata": {},
   "source": [
    "# 1. Pandas for tabular data [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e96c0",
   "metadata": {},
   "source": [
    "[Pandas](https://pandas.pydata.org/) is the defacto Python framework for working with tabular data, and it is supported by a large ecosystem of libraries, including integration with NumPy. Pandas provides two main data structures:\n",
    "\n",
    "- `DataFrame`: a 2-dimensional data structure often used to represent a table with rows and named columns. We can think of a `DataFrame` as a 2D numpy array with named columns.\n",
    "- `Series`: a 1-dimensional, **labelled** array, often used to represent a single column or row in a `DataFrame`\n",
    "\n",
    "We will primarily be using pandas to load in and analyze datasets before we pass them into our machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f9afc",
   "metadata": {},
   "source": [
    "To familiarize ourselves with working with the `pandas` library, we will load US Census data provided by the [folktables package](https://github.com/socialfoundations/folktables) and perform some fundamental pandas operations. \n",
    "\n",
    "From the creators of [folktables](https://github.com/socialfoundations/folktables):\n",
    "\n",
    "> Folktables is a Python package that provides access to datasets derived from the US Census, facilitating the benchmarking of machine learning algorithms. The package includes a suite of pre-defined prediction tasks in domains including income, employment, health, transportation, and housing, and also includes tools for creating new prediction tasks of interest in the US Census data ecosystem. The package additionally enables systematic studies of the effect of distribution shift, as each prediction task can be instantiated on datasets spanning multiple years and all states within the US.\n",
    "> \n",
    "> Why the name? Folktables is a neologism describing tabular data about individuals. **It emphasizes that data has the power to create and shape narratives about populations and challenges us to think carefully about the data we collect and use.**\n",
    "\n",
    "We will study the context and history around machine learning research using US Census data in the upcoming weeks, but for now, let's take a look at one particularly prominent dataset: the [ACS (American Community Survey)](https://www.census.gov/programs-surveys/acs) Income dataset, which contains socioeconomic data about individuals in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4338f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The standard import idiom for pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the ACS Income dataset\n",
    "income_df = pd.read_csv('~/COMSC-335/data/adult_reconstruction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4488d63",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "To load data from files manually, pandas provides various `pd.read_*` functions. For example, [pd.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) loads (comma-separated values) CSV files. See pandas' [I/O documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) for more options.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a0664",
   "metadata": {},
   "source": [
    "We often suffix variable names with `_df` to make it clear that we are working with a dataframe.\n",
    "\n",
    "For high-level inspection of the dataframe, we can use the following functions and attributes:\n",
    "\n",
    "- `df.head()`: returns the first 5 rows of the dataframe\n",
    "- `df.tail()`: returns the last 5 rows of the dataframe\n",
    "- `df.info()`: returns a summary of the dataframe, including the number of rows, columns, and the data types of each column\n",
    "- `df.columns`: returns the column names of the dataframe\n",
    "- `df.shape`: returns the number of rows and columns in the dataframe\n",
    "- `df.dtypes`: returns the data types of each column\n",
    "\n",
    "You can play around with the dataframe in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like numpy, pandas provides a `shape` attribute that returns a tuple of (num rows, num columns)\n",
    "income_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a012d",
   "metadata": {},
   "source": [
    "From above, we see that the dataframe has 49,531 rows and 14 columns. Here, each row represents an individual, and each column represents a feature of the individual. The machine learning task is to predict the `income` of an individual based on the other features.\n",
    "\n",
    "Taking a look at the columns, we see that there are a wide range of demographic and occupational features collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf4748",
   "metadata": {},
   "source": [
    "## Column selection and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bae53e",
   "metadata": {},
   "source": [
    "To select a single column, we can square bracket indexing with the name of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ca777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the 'education' column and prints the first 5 rows\n",
    "income_df['education'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febdf62",
   "metadata": {},
   "source": [
    "When initially exploring a dataset, it is often useful to see the unique values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d40938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'education' column\n",
    "income_df['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e359a",
   "metadata": {},
   "source": [
    "The square bracket indexing can be generalized to selecting multiple columns by passing a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ea62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects multiple columns and prints the last 10 rows\n",
    "cols = ['education', 'age', 'hours-per-week']\n",
    "income_df[cols].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7925d",
   "metadata": {},
   "source": [
    "We can also remove columns by using the `drop` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'relationship' column\n",
    "income_df = income_df.drop(columns=['relationship'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34cd20",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "Operations that modify the dataframe will return a new dataframe with the changes, and the original dataframe will not be modified. So if we want to modify the dataframe in place, we need to assign the result back to the original variable:\n",
    "\n",
    "```python\n",
    "income_df = income_df.drop(columns=['relationship'])\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c9a2",
   "metadata": {},
   "source": [
    "Just like NumPy, we can also use boolean indexing to select portions of the dataframe based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects individuals who are below the age of 30\n",
    "sel_df = income_df[income_df['age'] < 30]\n",
    "\n",
    "sel_df['age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dddeb",
   "metadata": {},
   "source": [
    "We can then use the `value_counts` function to get the frequency of each category in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value counts of the 'workclass' column for individuals below the age of 30\n",
    "print(sel_df['workclass'].value_counts())\n",
    "\n",
    "# Normalize=True to get the proportion of each category\n",
    "print(sel_df['workclass'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a5752",
   "metadata": {},
   "source": [
    "These boolean conditions can be combined using the `&` (AND), `|` (OR), and `~` (NOT) operators. Additionally, there are some special functions that can be used to select data based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining conditions: respondents who are less than 30 years old AND have non-zero capital-gain\n",
    "income_df[(income_df['age'] < 30) \n",
    "        & (income_df['capital-gain'] > 0)]\n",
    "\n",
    "# isin(): select rows where a column is in a list of values\n",
    "income_df[income_df['workclass'].isin(['Local-gov', 'State-gov'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84caab1f",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "To avoid errors, always use parentheses when combining conditions:\n",
    "\n",
    "- Incorrect: `df[column1 == value1 & column2 == value2]`\n",
    "- Correct: `df[(column1 == value1) & (column2 == value2)]`\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a5a92",
   "metadata": {},
   "source": [
    "**1.1**. Let's practice combining these operations. Select respondents who:\n",
    "\n",
    "- work **full-time**, defined as `hours-per-week >= 40` AND\n",
    "- have an `education` level in `['Bachelors', 'Masters', 'Doctorate']`\n",
    "\n",
    "Within this group, compute the counts of `occupation` using `value_counts()`. This column indicates the type of job the respondent has.\n",
    "\n",
    "\n",
    "\n",
    "How many individuals are in this group? What is the most common occupation in this group?\n",
    "\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here (make sure to use income_df for the dataframe)\n",
    "full_time_at_least_bachelors_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d303c6",
   "metadata": {},
   "source": [
    ":::{admonition} Solution (click to check once you've completed 1.1)\n",
    ":class: dropdown\n",
    "\n",
    "You should see that the group of respondents who work full-time and have an education level defined above has 9,463 individuals. The most common occupation in this group is `Prof-specialty` with 3,287 individuals, which corresponds to some professional specialization role (*not* an academic professor).\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac26c1",
   "metadata": {},
   "source": [
    "We can also use boolean logic to create new columns. For example, a common age cutoff used in US policy studies is 65, as it is the age that corresponds to \"seniors\" and is when people are eligible for Medicare health insurance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5bf141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'is_senior' that is 1 if the respondent is 65 or older, and 0 otherwise\n",
    "# .astype(int) converts the boolean values into 0s and 1s\n",
    "income_df['is_senior'] = (income_df['age'] >= 65).astype(int)\n",
    "\n",
    "income_df['is_senior'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c704491",
   "metadata": {},
   "source": [
    "We'll commonly apply this transformation to a continuous variable to create a binary indicator column, such as when we want to create a $y$ column for a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85cf9e",
   "metadata": {},
   "source": [
    "**1.2** Complete the `binarize_column` function below. This function takes a pandas Series and a cutpoint as arguments, and returns a Series with 1s and 0s indicating whether each element of the input series is greater than the cutpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d85d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_column(column: pd.Series, cutpoint: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Binarizes a column based on a cutpoint.\n",
    "\n",
    "    Args:\n",
    "        column (pd.Series): The column to binarize.\n",
    "        cutpoint (float): The cutpoint to use for binarization.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A column with 1s and 0s indicating whether each element of the input series is greater than the cutpoint.\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf34356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test binarize_column ####\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame({'A': [1, 2, 3, 4, 5]})\n",
    "    df['A_bin'] = binarize_column(df['A'], 2.5)\n",
    "    assert df.equals(pd.DataFrame({'A': [1, 2, 3, 4, 5], 'A_bin': [0, 0, 1, 1, 1]})), \"Binarization is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882e688",
   "metadata": {},
   "source": [
    "## Grouping and aggregation\n",
    "\n",
    "The [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) operation is a powerful tool for performing aggregations on subsets of the dataframe. We pass in one or more columns as the `by` argument, which then divides the original Dataframe based on the unique values of the column(s). We then often apply an **aggregation** function to each group, resulting in a new dataframe.\n",
    "\n",
    "You can think of it as:\n",
    "\n",
    "- split the dataframe into groups (based on unique values of a column), then\n",
    "- apply an aggregation (like `mean`, `median`, `size`, etc.) to each group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24844023",
   "metadata": {},
   "source": [
    "We can replicate `value_counts()` with `groupby()` and the `size()` aggregation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of people in each workclass category\n",
    "income_df.groupby(by='workclass').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33059fbb",
   "metadata": {},
   "source": [
    "We can also compute summary statistics like `mean()`, `std()`, `median()`, `min()`, `max()` on columns after grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, group by the 'workclass' column\n",
    "# Then, see the average number of work hours per week for each group\n",
    "income_df.groupby('workclass')['hours-per-week'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec887",
   "metadata": {},
   "source": [
    "To apply multiple aggregation functions, we can pass in a dictionary of columns as keys and functions as values to [agg()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03fe281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to apply the following aggregations:\n",
    "agg_dict = {\n",
    "    # mean, min, max for hours-per-week\n",
    "    'hours-per-week': ['mean', 'min', 'max'],\n",
    "    # median for education_num\n",
    "    'education-num': ['median']\n",
    "}\n",
    "\n",
    "# apply the aggregations, grouping by 'workclass'\n",
    "income_df.groupby('workclass').agg(agg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1dbc6",
   "metadata": {},
   "source": [
    "**1.3**. Compute a summary table that groups by the `is_senior` column and then computes the following aggregations:   \n",
    "\n",
    "- mean and median `income`\n",
    "- mean and median `capital-gain`\n",
    "\n",
    "What is the mean `income` for seniors? Do seniors have more or less median `capital-gain` than non-seniors?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5ea6a",
   "metadata": {},
   "source": [
    ":::{admonition} Solution (click to check once you've completed 1.3)\n",
    ":class: dropdown\n",
    "\n",
    "The groupby aggregation table should show that seniors have a mean income of about \\$29,670. Seniors have a higher median capital gain of \\$1,877, compared to non-seniors with a median capital gain of \\$1,029.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd91d9",
   "metadata": {},
   "source": [
    "## Processing categorical variables with `get_dummies`\n",
    "\n",
    "Many machine learning models expect purely numeric or binary features, but we've seen so far that our dataset has a number of categorical features that are encoded as strings.\n",
    "\n",
    "A common step taken to prepare data is **one-hot encoding**, where we convert a categorical column like `workclass` into a set of binary indicator columns: a new column is generated for each category within the column, called a **dummy variable**.\n",
    "\n",
    "In pandas, there is the [`pd.get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) function that can be used to perform this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28d8f4",
   "metadata": {},
   "source": [
    "Let's first see what the last 5 rows of the `workclass` column look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18457322",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(income_df['workclass'].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cf169",
   "metadata": {},
   "source": [
    "Then, we'll use `pd.get_dummies` to convert the `workclass` column into a set of binary indicator columns, one for each category in the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataframe with binary columns for each category in 'workclass'\n",
    "# dtype=int converts the boolean values into 0s and 1s\n",
    "workclass_dummy_df = pd.get_dummies(\n",
    "    data=income_df,\n",
    "    columns=['workclass'],\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "display(workclass_dummy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd09e1",
   "metadata": {},
   "source": [
    "Notice how in the `workclass_dummy_df`, each row has a 1 in the `workclass_category` column if the original `workclass` column had that category, and 0 otherwise. The columns argument can also be a list of columns to encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dda8c9",
   "metadata": {},
   "source": [
    "**1.4**. Use `pd.get_dummies()` to one-hot encode the `occupation` column in the `income_df` dataframe.\n",
    "\n",
    "What is the shape of the new dataframe compared to the original `income_df`? What does that tell us about the number of categories in the `occupation` column?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here\n",
    "occupation_dummy_df = None\n",
    "\n",
    "display(occupation_dummy_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36691482",
   "metadata": {},
   "source": [
    ":::{admonition} Solution (click to check once you've completed 1.4)\n",
    ":class: dropdown\n",
    "\n",
    "The shape of `occupation_dummy_df` is `(49531, 28)`, which has the same number of rows as `income_df` but 15 more columns. This tells us that there are 15 unique categories in the `occupation` column. We can also check this by using the `nunique()` function: `income_df['occupation'].nunique()`\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29718b5a",
   "metadata": {},
   "source": [
    ":::{admonition} More on pandas\n",
    "\n",
    "If you'd like to learn more about pandas operations, see [this quickstart guide](https://pandas.pydata.org/docs/user_guide/10min.html) and the associated links within it.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a31af5",
   "metadata": {},
   "source": [
    "# 2. Seaborn for data visualization [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27239f05",
   "metadata": {},
   "source": [
    "[seaborn](https://seaborn.pydata.org/tutorial/introduction.html) is one of the most popular libraries for creating visualizations in Python, as it is a higher-level library built on top of the more fundamental [matplotlib](https://matplotlib.org/) library.\n",
    "\n",
    "We'll use seaborn to visualize some relationships between variables in our income dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d5306",
   "metadata": {},
   "source": [
    "First, let's import seaborn using its standard import idiom, which abbreviates the name to `sns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4636a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899f8d8",
   "metadata": {},
   "source": [
    "Many seaborn plots follow a similar argument pattern, where we often pass in the following arguments:\n",
    "- `data`: the dataframe to plot\n",
    "- `x`: the column to plot on the x-axis\n",
    "- `y`: the column to plot on the y-axis, if applicable\n",
    "- `hue`: the column to use for color-coding the points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b876e",
   "metadata": {},
   "source": [
    "Seaborn has tight integration with pandas, which allows us to use pandas to filter and manipulate the data before passing it to seaborn. Let's generate a [sns.histplot](https://seaborn.pydata.org/generated/seaborn.histplot.html#seaborn.histplot) of `income` for individuals above the age of 30 who work in the private sector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdca1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # Select out data for individuals above the age of 30 who work in the private sector\n",
    "  above_30_private_df = income_df[\n",
    "        (income_df['age'] >= 30) \n",
    "      & (income_df['workclass'] == 'Private')\n",
    "  ]\n",
    "\n",
    "  # Generate the histogram with:\n",
    "  # Data: the below_30_private_df dataframe\n",
    "  # x-axis: 'income'\n",
    "  # y-axis: is the count of individuals in each bin so it does not need to be specified\n",
    "  sns.histplot(data=above_30_private_df, x='income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b1eb8",
   "metadata": {},
   "source": [
    "**2.1.** You should see that the distribution tails off to the right, with a large mass of individuals at the \\$100k mark. Speculate on why you think this mass is at \\$100k, keeping in mind that this data is generated from a survey of individuals in the US:\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24941d97",
   "metadata": {},
   "source": [
    "The most common machine learning task associated with this dataset is a binary classification task, where we predict whether the `income` of an individual is greater than or equal to \\$50k based on the given features. This dataset in particular has been extensively used historically as a benchmark for evaluating the [**fairness** of machine learning models](https://arxiv.org/pdf/2108.04884), such as whether the model is biased towards certain groups of individuals. We'll examine aspects of fairness in the upcoming weeks, but first let's get a sense of the data. Run the cell below to create a column that is a binary indicator of `income` >= $50k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df['income_>50k'] = income_df['income'] >= 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270cb3b",
   "metadata": {},
   "source": [
    "Complete the cell below to generate a histplot of `age` with the following parameters:\n",
    "\n",
    "- `data=income_df`: use the `income_df` dataframe for the plot\n",
    "- `x='age'`: plot the `age` column on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # TODO your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be201ab9",
   "metadata": {},
   "source": [
    "**2.2** Now, add the following parameters to your plot above:\n",
    "\n",
    "- `hue='income_>50k'`: this colors the histogram bars by the `income_>50k` column\n",
    "- `multiple='stack'`: stack the colored histograms on top of each other\n",
    "\n",
    "Briefly describe what you see in the relationship between `age` and `income_>50k` in the plot above. Some questions to consider:\n",
    "\n",
    "- Are there ages where there are very few individuals with incomes > \\$50k?\n",
    "- Is there a particular age, or age range where the proportion of individuals with incomes > \\$50k is higher?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24330d89",
   "metadata": {},
   "source": [
    ":::{admonition} Observations (click to check once you've completed 2.2)\n",
    ":class: dropdown\n",
    "\n",
    "In this data, you should see that there are very few individuals with incomes > \\$50k with ages <20, as well as in the ~75-80 age range.\n",
    "\n",
    "There is a peak in the proportion of individuals with incomes > \\$50k around age 47, and generally it looks like the older the individual, the more likely they are to have an income > \\$50k up until ~47, where it begins to slowly decline.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774cfb7",
   "metadata": {},
   "source": [
    "The top 4 most common occupations are (How would you check this using the pandas commands we saw earlier?):\n",
    "\n",
    "- Craft-repair\n",
    "- Prof-specialty\n",
    "- Exec-managerial\n",
    "- Adm-clerical\n",
    "\n",
    "\n",
    "Let's see how the proportion of individuals with incomes > \\$50k varies by these top 4 occupations with an [sns.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html#seaborn.countplot), which shows counts or percentages across categories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c35d0",
   "metadata": {},
   "source": [
    "**2.3** Complete the code below with the following parameters passed to countplot:\n",
    "\n",
    "- `data=top4_occupations_df`: use the `top4_occupations_df` dataframe for the plot\n",
    "- `x='occupation'`: plot the `occupation` column on the x-axis\n",
    "- `hue='income_>50k'`: this colors the bars by the `income_>50k` column\n",
    "- `stat='percent'`: show the percentage of individuals in each bar on the y-axis\n",
    "\n",
    "Which occupations seem to be most \"predictive\" of income > \\$50k?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    top4_occupations = [\n",
    "        'Craft-repair',\n",
    "        'Prof-specialty',\n",
    "        'Exec-managerial',\n",
    "        'Adm-clerical',\n",
    "    ]\n",
    "\n",
    "    # TODO select the rows in income_df where the 'occupation' column is in the top4_occupations list\n",
    "    top4_occupations_df = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3be4eb",
   "metadata": {},
   "source": [
    ":::{admonition} Observations (click to check once you've completed 2.3)\n",
    ":class: dropdown\n",
    "\n",
    "The `Exec-managerial` and `Prof-specialty` occupations have the highest percentage of individuals with incomes > \\$50k, while the `Adm-clerical` and `Craft-repair` occupations have the lowest.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de729017",
   "metadata": {},
   "source": [
    ":::{admonition} More on seaborn\n",
    ":class: note\n",
    "\n",
    "The following official resources are good references for seaborn if you'd like to learn more:\n",
    "\n",
    "- [seaborn plotting overview](https://seaborn.pydata.org/tutorial/function_overview.html)\n",
    "- [seaborn data structures](https://seaborn.pydata.org/tutorial/data_structure.html)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c5bcd",
   "metadata": {},
   "source": [
    "# 3. Probability primer [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80e147",
   "metadata": {},
   "source": [
    "We're now in the process of moving into machine learning **classification**, where the goal is to predict categories instead of continuous values. For this, we'll need to utilize some fundemental concepts from probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cc36e",
   "metadata": {},
   "source": [
    "## Chance events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0eccc",
   "metadata": {},
   "source": [
    "Probability is the mathematical framework that allows us to reason about randomness and chance. We often want to reason about discrete events that happen, such as whether a coin flip comes up heads or tails, whether it will rain tomorrow, or whether a machine learning model trained to recognize cats identifies an image as a cat or not.\n",
    "\n",
    "For all of these situations, we assign probabilities to an event $A$ that happens, $P(A)$. For example, a fair coin has a 50% chance of landing heads and a 50% chance of landing tails, so:\n",
    "\n",
    "$$P(\\text{heads}) = 0.5, \\quad P(\\text{tails}) = 0.5$$\n",
    "\n",
    "If we assign heads and tails to be numeric outcomes, e.g. $\\text{heads} = 1$ and $\\text{tails} = 0$, then the coin flip can be thought of as a **random variable** $Y$.\n",
    "\n",
    "In order for something to be considered a valid (discrete) random variable, the probability of each event must be between 0 and 1, and the sum of the probabilities of all events **must** equal 1. In the case of a fair coin, $P(Y=1) + P(Y=0) = 0.5 + 0.5 = 1$.\n",
    "\n",
    "We most frequently work in situations where the random variable is binary, for example:\n",
    "\n",
    "$$\n",
    "Y = \\begin{cases}\n",
    "    1 & \\text{image has a cat} \\\\\n",
    "    0 & \\text{image does not have a cat}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a74cc0",
   "metadata": {},
   "source": [
    "**3.1:** Suppose that we have a dataset of 1000 images, where 300 of them have cats in them. What is $P(Y=0)$, that is, the probability that a image does not have a cat?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821b222",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "\n",
    "The **expectation** of a random variable $Y$ is the average value of $Y$ over all possible outcomes. It is given by:\n",
    "\n",
    "$$\n",
    "E[Y] = \\sum_{y \\in \\mathcal{Y}} y P(Y=y)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{Y}$ is the set of all possible outcomes for $Y$. In the case of binary random variables, $\\mathcal{Y} = \\{0, 1\\}$ for the two possible outcomes.\n",
    "\n",
    "**3.2:** Continuing from our cat picture example above, what is the expectation $E[Y]$?\n",
    "\n",
    "$$\n",
    "E[Y] = TODO\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011a09f",
   "metadata": {},
   "source": [
    ":::{admonition} Solutions (click to check once you've completed 3.1-3.2)\n",
    ":class: dropdown\n",
    "\n",
    "**3.1:** $P(Y=0) = 1 - P(Y=1) = 1 - 0.3 = 0.7$\n",
    "\n",
    "**3.2:** $E[Y] = 0 \\cdot P(Y=0) + 1 \\cdot P(Y=1) = 0 \\cdot 0.7 + 1 \\cdot 0.3 = 0.3$\n",
    "\n",
    "An identity that is often used is that $E[Y] = P(Y=1)$ for binary random variables.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd035cd",
   "metadata": {},
   "source": [
    "## Joint and conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436bec3",
   "metadata": {},
   "source": [
    "We also often want to reason about the probability of two random variables occurring together, called a **joint probability**. If the two random variables $Y$ and $Z$ are binary, we can represent the joint probability using a **2x2 contingency table**:\n",
    " \n",
    "$$\n",
    "\\begin{array}{c|c|c}\n",
    "       & Y=0 & Y=1 \\\\\n",
    "\\hline\n",
    "Z=0 & \\text{count of } Z=0 \\text{ AND } Y=0 & \\text{count of } Z=0 \\text{ AND } Y=1 \\\\\n",
    "Z=1 & \\text{count of } Z=1 \\text{ AND } Y=0 & \\text{count of } Z=1 \\text{ AND } Y=1 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d132f49",
   "metadata": {},
   "source": [
    "Each entry in the contingency table is a count of the number of times the corresponding event occurs. For example, the entry in the first row and first column is the count of the number of times $Y=0$ and $Z=0$ occur together.\n",
    "\n",
    "Let $N$ be the total count of all the entries in the contingency table. We can take the **marginal probability** of $Y$ by summing over the columns of the contingency table:\n",
    "\n",
    "$$\n",
    "P(Y=0) = \\frac{(\\text{count of } Y=0 \\text{ and } Z=0) + (\\text{count of } Y=0 \\text{ and } Z=1)}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y=1) = \\frac{(\\text{count of } Y=1 \\text{ and } Z=0) + (\\text{count of } Y=1 \\text{ and } Z=1)}{N}\n",
    "$$\n",
    "\n",
    "Similarly, we can take the marginal probability of $Z$ by summing over the rows of the contingency table:\n",
    "\n",
    "$$\n",
    "P(Z=0) = \\frac{(\\text{count of } Z=0 \\text{ and } Y=0) + (\\text{count of } Z=0 \\text{ and } Y=1)}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Z=1) = \\frac{(\\text{count of } Z=1 \\text{ and } Y=0) + (\\text{count of } Z=1 \\text{ and } Y=1)}{N}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5426fe1",
   "metadata": {},
   "source": [
    "The **joint probability** of two events $Y$ and $Z$ occurring together, $P(Y=y, Z=z)$, is given by the entry in the contingency table for the corresponding row and column. For example, if $Y=0$ and $Z=1$, then:\n",
    "\n",
    "$$\n",
    "P(Y=0, Z=1) = \\frac{\\text{count of } Y=0 \\text{ and } Z=1}{N}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ce2fd",
   "metadata": {},
   "source": [
    "The conditional probability of $Y$ given $Z$, $P(Y=y \\mid Z=z)$, is given by the entry in the contingency table for the corresponding row and column, divided by the marginal probability of $Z$:\n",
    "\n",
    "$$\n",
    "P(Y=y \\mid Z=z) = \\frac{P(Y=y, Z=z)}{P(Z=z)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c392cda",
   "metadata": {},
   "source": [
    "Suppose we also know that some of our 1000 images also contain a cardboard box. We can represent this as a new random variable $Z$:\n",
    "\n",
    "$$\n",
    "Z = \\begin{cases}\n",
    "    1 & \\text{image contains a cardboard box} \\\\\n",
    "    0 & \\text{image does not contain a cardboard box}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cce82",
   "metadata": {},
   "source": [
    ":::{figure} images/uni_cat_box.jpg\n",
    ":scale: 50%\n",
    "\n",
    "A picture where $Z=1$ and $Y=1$. [Source: Uni the cat](https://www.instagram.com/unico_uniuni/)\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d5dca",
   "metadata": {},
   "source": [
    "\n",
    "We then have the following contingency table for our 1000 images:\n",
    "\n",
    " \n",
    "$$\n",
    "\\begin{array}{c|c|c}\n",
    " & Y=0 & Y=1 \\\\\n",
    "\\hline\n",
    "Z=0 & 600 & 100 \\\\\n",
    "Z=1 & 100 & 200 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d51063",
   "metadata": {},
   "source": [
    "**3.3**: Compute the following probabilities:\n",
    "\n",
    "- $P(Z=0) = TODO$\n",
    "- $P(Z=1) = TODO$\n",
    "- $P(Y=1 \\mid Z=0) = TODO$\n",
    "- $P(Y=1 \\mid Z=1) = TODO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd0a70",
   "metadata": {},
   "source": [
    "**3.4** Given your answers to 3.3, does the presence or does the absence of a cardboard box seem to more predictive of whether an image has a cat in it?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed006bf",
   "metadata": {},
   "source": [
    ":::{admonition} Solutions (click to check once you've completed 3.3 and 3.4)\n",
    ":class: dropdown\n",
    "\n",
    "- $P(Z=0) = 0.7$\n",
    "- $P(Z=1) = 0.3$\n",
    "- $P(Y=1 \\mid Z=0) = 1/7 \\approx 0.1429$\n",
    "- $P(Y=1 \\mid Z=1) = 2/3 \\approx 0.6667$\n",
    "\n",
    "To assess \"predictiveness\", we can look at the conditional probabilities of $Y$ given $Z$. The presence of a cardboard box seems to be more predictive of whether an image has a cat in it, as $P(Y=1 \\mid Z=1) > P(Y=1 \\mid Z=0)$. Additionally, $P(Y=1 \\mid Z=1) > P(Y=1)$, so the presence of a cardboard box seems to be a useful predictor of whether an image has a cat in it overall.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26acf33",
   "metadata": {},
   "source": [
    "# 4. Broadcasting and axis operations in NumPy [1.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299a270",
   "metadata": {},
   "source": [
    "As we scale up the number of features in our machine learning models, we can lean on broadcasting and axis operations in NumPy to make calculations across 2D arrays more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc3453",
   "metadata": {},
   "source": [
    "\n",
    "## Broadcasting\n",
    "\n",
    "[Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is one of the most powerful concepts in NumPy, but it is also one that takes some getting used to. So, let's review the idea of NumPy array shapes and what happens when using operators on differently shaped arrays.\n",
    "\n",
    "We saw on Worksheet 1 that arithmetic operations performed on arrays with the same shape are computed element-wise. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 1, 1])\n",
    "b = np.array([2, 4, 6])\n",
    "\n",
    "print(a + b) # Will print [3 5 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3ac17",
   "metadata": {},
   "source": [
    "Note that both arrays have the same shape (3,), which indicates that they are 1D arrays with 3 elements. **This is different from a 2D array of shape (3, 1)**, which is a vector with 3 rows and 1 column or a 2D array of shape (1, 3), which is a vector with 1 row and 3 columns.\n",
    "\n",
    "If NumPy encounters two arrays with different shapes, it will attempt to **broadcast** the arrays together. What happens is that NumPy will compare their shapes element-wise, starting from the rightmost dimension and working its way to the left. Two dimensions are compatible when:\n",
    "\n",
    "- they are equal, or\n",
    "- one of them is 1\n",
    "\n",
    "Once one of these conditions is met, the arithmetic operation is performed element-wise across that dimension. Below are some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684fd37",
   "metadata": {},
   "source": [
    "Below are some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is an array of shape (2, 3)\n",
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# b is an array of shape (2, 3)\n",
    "b = np.array([[1, 1, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "# Will print [[2 3 4], \n",
    "#             [5 6 7]]\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c3004",
   "metadata": {},
   "source": [
    "Above, working from the rightmost dimension, we see that both dimensions for `a` and `b` are equal:\n",
    "\n",
    "```python\n",
    "a       (2d array): 2 x 3\n",
    "b       (2d array): 2 x 3\n",
    "result  (2d array): 2 x 3\n",
    "```\n",
    "\n",
    "Therefore, the arithmetic operation is performed element-wise across the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62590074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is an array of shape (2, 3)\n",
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# c is an array of shape (3,)\n",
    "c = np.array([3, 2, 1])\n",
    "\n",
    "\n",
    "# Will print [[ 3  4  3], \n",
    "#             [12 10  6]]\n",
    "print(c * a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8747f29",
   "metadata": {},
   "source": [
    "Even though `c` has a different shape than `a`, we see that the dimension of `c` matches the last dimension of `a`, so the multiplication operation is performed element-wise across that dimension:\n",
    "\n",
    "```python\n",
    "c       (1d array):     3\n",
    "a       (2d array): 2 x 3\n",
    "result  (2d array): 2 x 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b63a9a",
   "metadata": {},
   "source": [
    "However, if we try to broadcast `a` with a 1D array of shape (2,), NumPy will raise a ValueError because the dimensions do not match:\n",
    "\n",
    "```python\n",
    "a       (2d array): 2 x 3\n",
    "d       (1d array):     2\n",
    "result: ValueError because of a dimension mismatch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a15327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is an array of shape (2, 3)\n",
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# d is an array of shape (2,)\n",
    "d = np.array([1, 2])\n",
    "\n",
    "# Will raise an Error because the dimensions do not match\n",
    "print(a + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd3016",
   "metadata": {},
   "source": [
    "**4.1** If `a` is an array of shape `(100,)` and `b` is an array of shape `(100, 10)`, what is the shape of `a - b` (or would it raise an error)?\n",
    "\n",
    "**Your response**: TODO\n",
    "\n",
    "**4.2** If `c` is an array of shape `(256, 16)` and `d` is an array of shape `(16,)`, what is the shape of `c + d` (or would it raise an error)?\n",
    "\n",
    "**Your response**: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fc524",
   "metadata": {},
   "source": [
    "**4.3** Work out what the output of the following arithmetic operation will be before checking your answer by running the code:\n",
    "\n",
    "```python\n",
    "\n",
    "# w is a 1D array of shape (2,)\n",
    "w = np.array([1, -2])\n",
    "\n",
    "# X is a 2D array of shape (3, 2)\n",
    "X = np.array([[0, 1],\n",
    "              [1, 0],\n",
    "              [2, 2]])\n",
    "\n",
    "print(w * X)\n",
    "```\n",
    "\n",
    "**Your response**: \n",
    "\n",
    "```python\n",
    "[[TODO]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cfc81",
   "metadata": {},
   "source": [
    "::: {admonition} Solutions (click to check once you've completed 4.1-4.3)\n",
    ":class: dropdown\n",
    "\n",
    "**4.1:** Since `a` is a 1D array and its shape does not match the rightmost dimension of `b`, this will raise a ValueError.\n",
    "\n",
    "**4.2:** Since `c`'s rightmost dimension matches `d`'s shape, the addition will be successful. The resulting array will have shape `(256, 16)`, with `d` being added to each **row** of `c`.\n",
    "\n",
    "**4.3:** The multiplication will result in each row of `X` being multiplied by the corresponding element of `w`:\n",
    "\n",
    "```python\n",
    "[[0, -2],\n",
    " [1, 0],\n",
    " [2, -4]]\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8004330",
   "metadata": {},
   "source": [
    "## Axis operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fd3f0",
   "metadata": {},
   "source": [
    "When working with 2D arrays, we often want to apply operations across rows or columns. NumPy provides a convenient way to do this using the `axis` parameter in many aggregation functions.\n",
    "\n",
    "For example, we have used the [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) function to sum all the elements in an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b179b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "\n",
    "# Will print 6\n",
    "print(np.sum(a)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21659b55",
   "metadata": {},
   "source": [
    "However, in 2D arrays we could also sum the elements across the rows or columns. By default, `np.sum` will sum all the elements in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Will print a single number, the sum of all the elements in b: 21\n",
    "print(np.sum(b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b850390",
   "metadata": {},
   "source": [
    "If we wanted to sum the column values for each row, we could specify the parameter `axis=1`, which tells `np.sum` to sum across the second dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132330d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Will print [6 15]\n",
    "print(np.sum(b, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628632a",
   "metadata": {},
   "source": [
    "This produces a new array of shape `(2,)`, which is the sum of each row in `b`.\n",
    "\n",
    "Similarly, if we wanted the sum of each column, we could specify the parameter `axis=0`, which tells `np.sum` to sum across the first dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1595a8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Will print [5 7 9]\n",
    "print(np.sum(b, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11c74e",
   "metadata": {},
   "source": [
    "This produces a new array of shape `(3,)`, which is the sum of each column in `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6324e",
   "metadata": {},
   "source": [
    "Another way we can think about the `axis` parameter is that we're telling NumPy which dimension to collapse in the resulting array:\n",
    "\n",
    "```python\n",
    "                           dim: 0   1\n",
    "b                   (2d array): 2 x 3\n",
    "# dimension 0 is collapsed\n",
    "np.sum(b, axis=0)   (1d array): _   3\n",
    "\n",
    "\n",
    "# dimension 1 is collapsed\n",
    "np.sum(b, axis=1)   (1d array): 2   _\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ca8de",
   "metadata": {},
   "source": [
    "**4.4.** Suppose that `X` is a 2D array of shape `(n, p)`. Write the line of code that computes the mean of each column of `X`, which results in an array of shape `(p,)`:\n",
    "\n",
    "**Your response**: `TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e2f1b",
   "metadata": {},
   "source": [
    ":::{admonition} Solution (click to check once you've completed 4.4)\n",
    ":class: dropdown\n",
    "We want to collapse the dimension that corresponds to the number of rows, so we specify `axis=0`: `np.mean(X, axis=0)`\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b9413",
   "metadata": {},
   "source": [
    "Finally, let's put broadcasting and axis operations together to more efficiently compute predictions from our linear regression model.\n",
    "\n",
    "With one feature, our linear regression model prediction for a given example $x_i$ has the form:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w_0 + w_1 x_{i,1}\n",
    "$$\n",
    "\n",
    "With two features on Homework 1, our linear regression model had the form:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w_0 + w_1 x_{i,1} + w_2 x_{i,2}\n",
    "$$\n",
    "\n",
    "While we could compute the weighted sum for each row of `X` and then add the bias term manually, we'd like to generalize this to any number of features $p$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w_0 + \\sum_{j=1}^{p} w_j x_{i,j}\n",
    "$$\n",
    "\n",
    "Furthermore, we'd like to compute the predictions for all examples in `X` at once:\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\hat{y}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "w_0 + \\sum_{j=1}^{p} w_j x_{1,j} \\\\\n",
    "w_0 + \\sum_{j=1}^{p} w_j x_{2,j} \\\\\n",
    "\\vdots \\\\\n",
    "w_0 + \\sum_{j=1}^{p} w_j x_{n,j}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b8e7a",
   "metadata": {},
   "source": [
    "Complete the function below to compute the predictions for all examples in `X` at once:\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "This can be done using a single line of code. First, we can use broadcasting to compute the product of `X` and `w`. Then we can sum across the appropriate axis to compute $\\sum_{j=1}^{p} w_j x_{i,j}$ for each row. Finally, since $w_0$ is a scalar, we can just add it to the result.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32243710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_predictions(X: np.ndarray, w: np.ndarray, w0: float) -> np.ndarray:\n",
    "    \"\"\"Efficiently compute the predictions for all examples in X at once.\n",
    "\n",
    "    Args:\n",
    "        X: data examples of shape (n, p)\n",
    "        w: weights of shape (p,)\n",
    "        w0: scalar intercept term\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (n,) where each element is the prediction for the corresponding example in X\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[1., 2.],\n",
    "                  [3., 4.],\n",
    "                  [5., 6.]])\n",
    "    w = np.array([0.1, -0.2])\n",
    "    w0 = 0.5\n",
    "\n",
    "    predictions = linreg_predictions(X, w, w0)\n",
    "    assert predictions.shape == (3,), \"predictions should have shape (n,)\"\n",
    "    assert np.allclose(predictions, np.array([0.2, 0.0, -0.2])), \"predictions are not correct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea14f9",
   "metadata": {},
   "source": [
    "# 5. Reflection [0.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778d335",
   "metadata": {},
   "source": [
    "**5.1** How much time did it take you to complete this worksheet?\n",
    "\n",
    "**Your Response**: TODO\n",
    "\n",
    "**5.2** What is one thing you have a better understanding of after completing this worksheet and going though the class content this week? This could be about the concepts, the reading, or the code.\n",
    "\n",
    "**Your Response**: TODO\n",
    "\n",
    "**5.3** What questions or points of confusion do you have about the material covered in the past week of class?\n",
    "\n",
    "**Your Response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39b45",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "\n",
    "- Portions of this worksheet are adapted from Bhargavi's study notes on pandas.\n",
    "- Some exercises are adapted from [Deisenroth 2020: Mathematics for Machine Learning](https://mml-book.github.io/).\n",
    "- Folktables was introduced by [Ding et al. 2021: Retiring Adult: New Datasets for Fair Machine Learning](https://arxiv.org/abs/2108.04884)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
